<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Expert Storage Guide - 30 Years of Storage Wisdom for GPU-NVMe</title>
    <style>
        :root {
            --bg-void: #030308;
            --bg-deep: #0a0a12;
            --bg-surface: #12121c;
            --bg-elevated: #1a1a28;
            --text-primary: #e4e4ed;
            --text-secondary: #9494a8;
            --text-muted: #5c5c72;
            --accent-cyber: #00ffaa;
            --accent-electric: #00d4ff;
            --accent-plasma: #a855f7;
            --accent-solar: #fbbf24;
            --accent-ember: #ff6b6b;
            --gradient-cyber: linear-gradient(135deg, #00ffaa 0%, #00d4ff 100%);
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-void);
            color: var(--text-primary);
            line-height: 1.7;
        }

        .nav {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            height: 64px;
            background: rgba(3, 3, 8, 0.9);
            backdrop-filter: blur(20px);
            border-bottom: 1px solid rgba(255,255,255,0.05);
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 0 40px;
            z-index: 100;
        }

        .nav a {
            color: var(--text-secondary);
            text-decoration: none;
            font-size: 0.9rem;
            font-weight: 500;
            transition: color 0.2s;
        }

        .nav a:hover { color: var(--accent-cyber); }
        .nav-title { color: var(--text-primary); font-weight: 600; }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 100px 24px 80px;
        }

        .page-header {
            text-align: center;
            margin-bottom: 60px;
        }

        .page-number {
            font-size: 6rem;
            font-weight: 900;
            line-height: 1;
            background: linear-gradient(180deg, rgba(255,255,255,0.08) 0%, rgba(255,255,255,0.01) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: -30px;
        }

        .page-title {
            font-size: 2.5rem;
            font-weight: 700;
            background: var(--gradient-cyber);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 16px;
        }

        .page-subtitle {
            font-size: 1.1rem;
            color: var(--text-secondary);
            max-width: 700px;
            margin: 0 auto;
        }

        .section {
            background: var(--bg-surface);
            border: 1px solid rgba(255,255,255,0.05);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 32px;
        }

        .section-title {
            font-size: 1.4rem;
            font-weight: 700;
            color: var(--accent-solar);
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 1px solid rgba(251, 191, 36, 0.2);
        }

        .subsection-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--accent-cyber);
            margin: 24px 0 12px;
        }

        h2 { color: var(--accent-solar); margin: 32px 0 16px; }
        h3 { color: var(--accent-cyber); margin: 24px 0 12px; }
        h4 { color: var(--accent-electric); margin: 20px 0 10px; }

        p { color: var(--text-secondary); margin-bottom: 16px; }
        
        ul, ol { color: var(--text-secondary); margin: 16px 0 16px 24px; }
        li { margin-bottom: 8px; }

        a { color: var(--accent-electric); }
        a:hover { color: var(--accent-cyber); }

        strong { color: var(--text-primary); }
        em { color: var(--accent-plasma); font-style: normal; }

        .code-block, pre {
            background: #0d1117;
            border: 1px solid #30363d;
            border-radius: 12px;
            padding: 20px;
            margin: 16px 0;
            overflow-x: auto;
            font-family: 'JetBrains Mono', 'Consolas', monospace;
            font-size: 0.85rem;
            line-height: 1.6;
            white-space: pre;
            color: #e6edf3;
        }

        code {
            background: rgba(255,255,255,0.1);
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'JetBrains Mono', 'Consolas', monospace;
            font-size: 0.9em;
            color: var(--accent-cyber);
        }

        pre code {
            background: none;
            padding: 0;
            color: inherit;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: var(--bg-elevated);
            border-radius: 12px;
            overflow: hidden;
        }

        th {
            background: rgba(0, 255, 170, 0.1);
            color: var(--accent-cyber);
            padding: 14px 16px;
            text-align: left;
            font-weight: 600;
            border-bottom: 2px solid rgba(0, 255, 170, 0.2);
        }

        td {
            padding: 12px 16px;
            border-bottom: 1px solid rgba(255,255,255,0.05);
            color: var(--text-secondary);
        }

        tr:hover { background: rgba(255,255,255,0.02); }

        .info-box, .warning-box, .note-box {
            padding: 20px 24px;
            border-radius: 12px;
            margin: 20px 0;
        }

        .warning-box {
            background: rgba(255, 107, 107, 0.1);
            border-left: 4px solid var(--accent-ember);
        }

        .info-box {
            background: rgba(0, 212, 255, 0.1);
            border-left: 4px solid var(--accent-electric);
        }

        .note-box, .success-box {
            background: rgba(0, 255, 170, 0.1);
            border-left: 4px solid var(--accent-cyber);
        }

        .nav-footer {
            display: flex;
            justify-content: space-between;
            margin-top: 48px;
            padding-top: 32px;
            border-top: 1px solid rgba(255,255,255,0.05);
        }

        .nav-btn {
            display: flex;
            align-items: center;
            gap: 12px;
            padding: 14px 20px;
            background: var(--bg-surface);
            border: 1px solid rgba(255,255,255,0.1);
            border-radius: 10px;
            color: var(--text-primary);
            text-decoration: none;
            transition: all 0.3s;
        }

        .nav-btn:hover {
            border-color: var(--accent-cyber);
            background: var(--bg-elevated);
            color: var(--text-primary);
        }

        .expert-badge {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            background: linear-gradient(135deg, #ff6b6b 0%, #fbbf24 100%);
            color: #000;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.75rem;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 16px;
        }

        /* Emoji fixes */
        .emoji { font-style: normal; }

        @media (max-width: 768px) {
            .nav { padding: 0 20px; }
            .container { padding: 80px 16px 60px; }
            .section { padding: 24px; }
            .page-title { font-size: 1.8rem; }
            .nav-footer { flex-direction: column; gap: 12px; }
        }
</style>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <nav class="nav-header">
            <a href="index.html">Ã¢â€ Â Home</a>
            <span style="color: var(--text-secondary);">Expert Storage Guide</span>
            <a href="11_ml_frameworks.html">ML Frameworks Ã¢â€ â€™</a>
        </nav>

        <div class="hero">
            <span class="hero-badge">Ã°Å¸Ââ€  30 Years of Storage Expertise</span>
            <h1>Expert Storage Guide for GPU-NVMe Integration</h1>
            <p>Hard-won lessons from SCSI to SAS to NVMe. What actually matters for AI workloads, 
            and the mistakes I've seen companies make repeatedly over three decades.</p>
        </div>

        <div class="toc">
            <h3>Ã°Å¸â€œâ€˜ Expert Topics Covered</h3>
            <div class="toc-grid">
                <a href="#ml-frameworks">ML Framework Storage Patterns</a>
                <a href="#distributed-training">Distributed Training I/O</a>
                <a href="#monitoring">Production Monitoring Stack</a>
                <a href="#virtualization">Virtualization & Multi-tenancy</a>
                <a href="#nvme-advanced">Advanced NVMe Features</a>
                <a href="#power-management">Power & Thermal Management</a>
                <a href="#cloud-deployments">Cloud Provider Specifics</a>
                <a href="#next-gen">Next-Gen Hardware Planning</a>
            </div>
        </div>

        <!-- Critical: Data Placement Truth Table -->
        <section class="section" style="border: 2px solid #ff6b6b;">
            <h2 class="section-title" style="color: #ff6b6b;">Ã¢Å¡Â Ã¯Â¸Â Critical: Where Data Actually Belongs</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â The #1 Mistake:</strong> Treating SSDs like they're fast RAM. They're not. 
                This table shows what goes where. Get this wrong and no amount of GDS or optimization will save you.
            </div>

            <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                <thead>
                    <tr style="background: #1a1a2e; color: #00ff88;">
                        <th style="padding: 12px; text-align: left;">Data Type</th>
                        <th style="padding: 12px; text-align: left;">Correct Home</th>
                        <th style="padding: 12px; text-align: left;">Can Spill to SSD?</th>
                        <th style="padding: 12px; text-align: left;">Notes</th>
                    </tr>
                </thead>
                <tbody style="color: #ccc;">
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;"><strong>KV Cache</strong></td>
                        <td style="padding: 12px; color: #00ff88;">HBM (GPU memory)</td>
                        <td style="padding: 12px; color: #ff6b6b;">Emergency only</td>
                        <td style="padding: 12px;">Paging KV to SSD kills inference latency. Treat as last resort.</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;"><strong>Activations</strong></td>
                        <td style="padding: 12px; color: #00ff88;">HBM</td>
                        <td style="padding: 12px; color: #ffaa00;">With recompute</td>
                        <td style="padding: 12px;">Activation checkpointing trades compute for memory.</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;"><strong>Hot Embeddings</strong></td>
                        <td style="padding: 12px; color: #00ff88;">HBM/DRAM</td>
                        <td style="padding: 12px; color: #ffaa00;">Cold tier only</td>
                        <td style="padding: 12px;">Cache hit rate must be >90% or performance collapses.</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;"><strong>Training Samples</strong></td>
                        <td style="padding: 12px; color: #00d4ff;">NVMe / Parallel FS</td>
                        <td style="padding: 12px; color: #00ff88;">Yes (primary)</td>
                        <td style="padding: 12px;">Sequential prefetch. Pack samples (WebDataset/tar). 256KB-8MB reads.</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;"><strong>Model Checkpoints</strong></td>
                        <td style="padding: 12px; color: #00d4ff;">NVMe / Parallel FS</td>
                        <td style="padding: 12px; color: #00ff88;">Yes (primary)</td>
                        <td style="padding: 12px;">Large sequential writes. Async + incremental. GB-TB per checkpoint.</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;"><strong>Model Weights</strong></td>
                        <td style="padding: 12px; color: #00ff88;">HBM (after load)</td>
                        <td style="padding: 12px; color: #00d4ff;">Source: NVMe</td>
                        <td style="padding: 12px;">Loaded once at startup. Sequential 8MB-1GB chunks.</td>
                    </tr>
                </tbody>
            </table>

            <h3 class="subsection-title">Memory/Storage Tier Latency (Realistic)</h3>
            <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                <thead>
                    <tr style="background: #1a1a2e; color: #00ff88;">
                        <th style="padding: 12px; text-align: left;">Tier</th>
                        <th style="padding: 12px; text-align: left;">Access Granularity</th>
                        <th style="padding: 12px; text-align: left;">Typical Latency</th>
                        <th style="padding: 12px; text-align: left;">Bandwidth</th>
                    </tr>
                </thead>
                <tbody style="color: #ccc;">
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;">GPU HBM</td>
                        <td style="padding: 12px;">32-128B</td>
                        <td style="padding: 12px; color: #00ff88;">~100s ns</td>
                        <td style="padding: 12px;">3-8 TB/s</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;">Host DRAM</td>
                        <td style="padding: 12px;">64B</td>
                        <td style="padding: 12px; color: #00ff88;">~100 ns</td>
                        <td style="padding: 12px;">200-400 GB/s</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;">CXL.mem Expansion</td>
                        <td style="padding: 12px;">64B</td>
                        <td style="padding: 12px; color: #ffaa00;">~200-400+ ns (varies)</td>
                        <td style="padding: 12px;">64-128 GB/s</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;">Local NVMe SSD</td>
                        <td style="padding: 12px;">4KB+ (practical)</td>
                        <td style="padding: 12px; color: #ff6b6b;">70-150 Âµs</td>
                        <td style="padding: 12px;">6-14 GB/s</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #333;">
                        <td style="padding: 12px;">NVMe-oF (RDMA)</td>
                        <td style="padding: 12px;">4KB+</td>
                        <td style="padding: 12px; color: #ff6b6b;">120-200 Âµs</td>
                        <td style="padding: 12px;">25-100 GB/s (fabric)</td>
                    </tr>
                </tbody>
            </table>

            <div class="info-box" style="background: #1a2a1a; border-left: 4px solid #00ff88;">
                <strong style="color: #00ff88;">âœ“ Key Insight:</strong> The gap between HBM (~100ns) and NVMe (~100Âµs) is 
                <strong>1000x</strong>. No software optimization bridges this. Design your data placement correctly first.
            </div>
        </section>

        <!-- Section 1: ML Framework Storage Patterns -->
        <section class="section" id="ml-frameworks">
            <h2 class="section-title">Ã°Å¸Â§Â  ML Framework Storage Patterns</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â 30-Year Veteran Insight:</strong> Every ML framework thinks it's special. They're not. 
                Underneath the Python wrappers, they all do the same thing: stream large sequential reads for training data, 
                random reads for inference, and massive checkpoint writes. The storage doesn't care if it's PyTorch or TensorFlow.
                What matters is understanding <em>when</em> they access storage and <em>how much</em> they buffer.
            </div>

            <h3 class="subsection-title">PyTorch Storage Patterns</h3>
            
            <p>PyTorch's DataLoader is where most storage I/O originates. Understanding its internals is critical for optimization:</p>

            <pre><code><span style="color: #6a9955;"># PyTorch DataLoader with GPU-optimized storage access</span>
<span style="color: #c586c0;">import</span> torch
<span style="color: #c586c0;">from</span> torch.utils.data <span style="color: #c586c0;">import</span> DataLoader, Dataset
<span style="color: #c586c0;">import</span> kvikio
<span style="color: #c586c0;">import</span> cupy <span style="color: #c586c0;">as</span> cp

<span style="color: #c586c0;">class</span> <span style="color: #4ec9b0;">GPUDirectDataset</span>(Dataset):
    <span style="color: #ce9178;">"""Dataset that uses GDS for direct GPU loading"""</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__init__</span>(self, file_paths, chunk_size=<span style="color: #b5cea8;">64</span>*<span style="color: #b5cea8;">1024</span>*<span style="color: #b5cea8;">1024</span>):
        self.file_paths = file_paths
        self.chunk_size = chunk_size
        <span style="color: #6a9955;"># Pre-allocate GPU buffer pool to avoid allocation overhead</span>
        self.buffer_pool = [cp.empty(chunk_size, dtype=cp.uint8) 
                          <span style="color: #c586c0;">for</span> _ <span style="color: #c586c0;">in</span> range(<span style="color: #b5cea8;">4</span>)]  <span style="color: #6a9955;"># 4-deep buffer</span>
        self.pool_idx = <span style="color: #b5cea8;">0</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__getitem__</span>(self, idx):
        <span style="color: #6a9955;"># Round-robin through buffer pool</span>
        buf = self.buffer_pool[self.pool_idx % len(self.buffer_pool)]
        self.pool_idx += <span style="color: #b5cea8;">1</span>
        
        <span style="color: #6a9955;"># GDS read directly to GPU memory</span>
        <span style="color: #c586c0;">with</span> kvikio.CuFile(self.file_paths[idx], <span style="color: #ce9178;">"r"</span>) <span style="color: #c586c0;">as</span> f:
            bytes_read = f.read(buf)
        
        <span style="color: #6a9955;"># Return as PyTorch tensor (zero-copy from CuPy)</span>
        <span style="color: #c586c0;">return</span> torch.as_tensor(buf[:bytes_read], device=<span style="color: #ce9178;">'cuda'</span>)

<span style="color: #6a9955;"># Optimal DataLoader configuration for GDS</span>
loader = DataLoader(
    GPUDirectDataset(file_list),
    batch_size=<span style="color: #b5cea8;">32</span>,
    num_workers=<span style="color: #b5cea8;">0</span>,        <span style="color: #6a9955;"># CRITICAL: 0 workers for GDS (GPU context issue)</span>
    pin_memory=<span style="color: #569cd6;">False</span>,    <span style="color: #6a9955;"># Not needed - already on GPU</span>
    prefetch_factor=<span style="color: #b5cea8;">2</span>,   <span style="color: #6a9955;"># Keep pipeline full</span>
)</code></pre>

            <div class="info-box warning">
                <strong>Ã¢Å¡Â Ã¯Â¸Â Critical PyTorch Pitfall:</strong> Using <code class="inline-code">num_workers > 0</code> with GDS 
                will silently fall back to CPU bounce buffers. Each worker creates a new process without GPU context inheritance.
                I've seen teams wonder why their "GDS-enabled" training was still slow â€” this was why.
            </div>

            <h3 class="subsection-title">TensorFlow Storage Patterns</h3>
            
            <p>TensorFlow's tf.data API has different semantics that affect storage access:</p>

            <pre><code><span style="color: #6a9955;"># TensorFlow with GPU-optimized data pipeline</span>
<span style="color: #c586c0;">import</span> tensorflow <span style="color: #c586c0;">as</span> tf

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">create_optimized_dataset</span>(file_pattern, batch_size=<span style="color: #b5cea8;">32</span>):
    <span style="color: #ce9178;">"""TensorFlow dataset optimized for GPU storage access"""</span>
    
    <span style="color: #6a9955;"># Use TFRecord for optimal sequential access</span>
    files = tf.data.Dataset.list_files(file_pattern, shuffle=<span style="color: #569cd6;">True</span>)
    
    dataset = files.interleave(
        <span style="color: #c586c0;">lambda</span> x: tf.data.TFRecordDataset(
            x,
            compression_type=<span style="color: #ce9178;">''</span>,           <span style="color: #6a9955;"># No compression - let NVMe fly</span>
            buffer_size=<span style="color: #b5cea8;">256</span>*<span style="color: #b5cea8;">1024</span>*<span style="color: #b5cea8;">1024</span>,  <span style="color: #6a9955;"># 256MB buffer per file</span>
            num_parallel_reads=<span style="color: #b5cea8;">4</span>          <span style="color: #6a9955;"># Match NVMe queue depth</span>
        ),
        cycle_length=<span style="color: #b5cea8;">16</span>,                  <span style="color: #6a9955;"># Files to read in parallel</span>
        block_length=<span style="color: #b5cea8;">1</span>,                   <span style="color: #6a9955;"># Records per file before switching</span>
        num_parallel_calls=tf.data.AUTOTUNE,
        deterministic=<span style="color: #569cd6;">False</span>               <span style="color: #6a9955;"># Allow reordering for throughput</span>
    )
    
    <span style="color: #6a9955;"># Parse and prefetch to GPU</span>
    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    <span style="color: #6a9955;"># CRITICAL: Copy to GPU in pipeline, not in training loop</span>
    dataset = dataset.apply(tf.data.experimental.copy_to_device(<span style="color: #ce9178;">'/GPU:0'</span>))
    dataset = dataset.prefetch(<span style="color: #b5cea8;">1</span>)  <span style="color: #6a9955;"># Prefetch on GPU</span>
    
    <span style="color: #c586c0;">return</span> dataset

<span style="color: #6a9955;"># Storage format recommendations by use case</span>
STORAGE_FORMAT_GUIDE = {
    <span style="color: #ce9178;">'image_training'</span>: {
        <span style="color: #ce9178;">'format'</span>: <span style="color: #ce9178;">'TFRecord'</span>,
        <span style="color: #ce9178;">'compression'</span>: <span style="color: #569cd6;">None</span>,      <span style="color: #6a9955;"># Images already compressed</span>
        <span style="color: #ce9178;">'record_size'</span>: <span style="color: #ce9178;">'1-10MB'</span>,  <span style="color: #6a9955;"># Batch images together</span>
        <span style="color: #ce9178;">'reason'</span>: <span style="color: #ce9178;">'Sequential access, good for NVMe prefetch'</span>
    },
    <span style="color: #ce9178;">'nlp_training'</span>: {
        <span style="color: #ce9178;">'format'</span>: <span style="color: #ce9178;">'TFRecord or Parquet'</span>,
        <span style="color: #ce9178;">'compression'</span>: <span style="color: #ce9178;">'ZSTD level 1'</span>,  <span style="color: #6a9955;"># Text compresses well</span>
        <span style="color: #ce9178;">'record_size'</span>: <span style="color: #ce9178;">'64KB-1MB'</span>,
        <span style="color: #ce9178;">'reason'</span>: <span style="color: #ce9178;">'Token sequences are small, compression helps'</span>
    },
    <span style="color: #ce9178;">'inference_serving'</span>: {
        <span style="color: #ce9178;">'format'</span>: <span style="color: #ce9178;">'Memory-mapped or raw binary'</span>,
        <span style="color: #ce9178;">'compression'</span>: <span style="color: #569cd6;">None</span>,
        <span style="color: #ce9178;">'alignment'</span>: <span style="color: #ce9178;">'4KB aligned'</span>,  <span style="color: #6a9955;"># Match NVMe LBA size</span>
        <span style="color: #ce9178;">'reason'</span>: <span style="color: #ce9178;">'Random access, need O_DIRECT compatibility'</span>
    }
}</code></pre>

            <h3 class="subsection-title">JAX Storage Patterns</h3>
            
            <p>JAX's functional paradigm creates unique storage challenges, especially with its JIT compilation:</p>

            <pre><code><span style="color: #6a9955;"># JAX with optimized storage access</span>
<span style="color: #c586c0;">import</span> jax
<span style="color: #c586c0;">import</span> jax.numpy <span style="color: #c586c0;">as</span> jnp
<span style="color: #c586c0;">from</span> jax.experimental <span style="color: #c586c0;">import</span> io_callback
<span style="color: #c586c0;">import</span> kvikio

<span style="color: #6a9955;"># JAX doesn't have native GPU storage - need io_callback</span>
<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">load_batch_to_gpu</span>(file_paths, batch_indices):
    <span style="color: #ce9178;">"""Load data directly to GPU for JAX"""</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">_load_impl</span>(paths, indices):
        <span style="color: #6a9955;"># This runs on CPU but outputs to GPU</span>
        buffers = []
        <span style="color: #c586c0;">for</span> i, path <span style="color: #c586c0;">in</span> zip(indices, paths):
            <span style="color: #c586c0;">with</span> kvikio.CuFile(path.decode(), <span style="color: #ce9178;">"r"</span>) <span style="color: #c586c0;">as</span> f:
                gpu_buf = jax.device_put(jnp.empty(f.size(), dtype=jnp.uint8))
                f.read(gpu_buf)
                buffers.append(gpu_buf)
        <span style="color: #c586c0;">return</span> jnp.stack(buffers)
    
    <span style="color: #6a9955;"># io_callback allows I/O in JIT-compiled functions</span>
    <span style="color: #c586c0;">return</span> io_callback(
        _load_impl,
        jax.ShapeDtypeStruct((len(batch_indices), MAX_SIZE), jnp.uint8),
        file_paths,
        batch_indices
    )

<span style="color: #6a9955;"># JAX checkpointing with orbax (recommended)</span>
<span style="color: #c586c0;">from</span> orbax.checkpoint <span style="color: #c586c0;">import</span> CheckpointManager, PyTreeCheckpointer

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">setup_jax_checkpointing</span>(checkpoint_dir, max_to_keep=<span style="color: #b5cea8;">3</span>):
    <span style="color: #ce9178;">"""Optimized JAX checkpointing configuration"""</span>
    
    options = CheckpointManagerOptions(
        max_to_keep=max_to_keep,
        save_interval_steps=<span style="color: #b5cea8;">1000</span>,
        <span style="color: #6a9955;"># Async checkpointing - doesn't block training</span>
        async_options=AsyncOptions(
            timeout_secs=<span style="color: #b5cea8;">300</span>,
            barrier_sync_timeout_secs=<span style="color: #b5cea8;">60</span>
        )
    )
    
    <span style="color: #c586c0;">return</span> CheckpointManager(
        checkpoint_dir,
        PyTreeCheckpointer(),
        options=options
    )</code></pre>

            <h3 class="subsection-title">DeepSpeed Storage Patterns</h3>
            
            <div class="info-box critical">
                <strong>Ã°Å¸Å¡Â¨ DeepSpeed is Critical for LLM Training:</strong> If you're training models > 10B parameters, 
                you're almost certainly using DeepSpeed or FSDP. DeepSpeed's ZeRO stages fundamentally change storage 
                access patterns â€” understanding this is non-negotiable.
            </div>

            <pre><code><span style="color: #6a9955;"># DeepSpeed ZeRO-3 with NVMe offload configuration</span>
<span style="color: #6a9955;"># ds_config.json</span>
{
    <span style="color: #ce9178;">"zero_optimization"</span>: {
        <span style="color: #ce9178;">"stage"</span>: <span style="color: #b5cea8;">3</span>,
        <span style="color: #ce9178;">"offload_optimizer"</span>: {
            <span style="color: #ce9178;">"device"</span>: <span style="color: #ce9178;">"nvme"</span>,
            <span style="color: #ce9178;">"nvme_path"</span>: <span style="color: #ce9178;">"/mnt/nvme_raid"</span>,
            <span style="color: #ce9178;">"buffer_count"</span>: <span style="color: #b5cea8;">4</span>,           <span style="color: #6a9955;">// Pipeline depth</span>
            <span style="color: #ce9178;">"buffer_size"</span>: <span style="color: #b5cea8;">1073741824</span>,  <span style="color: #6a9955;">// 1GB per buffer</span>
            <span style="color: #ce9178;">"fast_init"</span>: <span style="color: #569cd6;">true</span>,
            <span style="color: #ce9178;">"pin_memory"</span>: <span style="color: #569cd6;">true</span>
        },
        <span style="color: #ce9178;">"offload_param"</span>: {
            <span style="color: #ce9178;">"device"</span>: <span style="color: #ce9178;">"nvme"</span>,
            <span style="color: #ce9178;">"nvme_path"</span>: <span style="color: #ce9178;">"/mnt/nvme_raid"</span>,
            <span style="color: #ce9178;">"buffer_count"</span>: <span style="color: #b5cea8;">4</span>,
            <span style="color: #ce9178;">"buffer_size"</span>: <span style="color: #b5cea8;">1073741824</span>
        },
        <span style="color: #ce9178;">"aio"</span>: {
            <span style="color: #ce9178;">"block_size"</span>: <span style="color: #b5cea8;">1048576</span>,      <span style="color: #6a9955;">// 1MB - optimal for NVMe</span>
            <span style="color: #ce9178;">"queue_depth"</span>: <span style="color: #b5cea8;">32</span>,          <span style="color: #6a9955;">// Match SSD queue depth</span>
            <span style="color: #ce9178;">"thread_count"</span>: <span style="color: #b5cea8;">4</span>,          <span style="color: #6a9955;">// I/O threads</span>
            <span style="color: #ce9178;">"single_submit"</span>: <span style="color: #569cd6;">false</span>,     <span style="color: #6a9955;">// Batch submissions</span>
            <span style="color: #ce9178;">"overlap_events"</span>: <span style="color: #569cd6;">true</span>      <span style="color: #6a9955;">// Async I/O</span>
        }
    },
    <span style="color: #ce9178;">"checkpoint"</span>: {
        <span style="color: #ce9178;">"tag_validation"</span>: <span style="color: #ce9178;">"warn"</span>,
        <span style="color: #ce9178;">"load_universal"</span>: <span style="color: #569cd6;">true</span>,
        <span style="color: #ce9178;">"use_node_local_storage"</span>: <span style="color: #569cd6;">true</span>,  <span style="color: #6a9955;">// Each node saves locally first</span>
        <span style="color: #ce9178;">"parallel_write"</span>: {
            <span style="color: #ce9178;">"pipeline_read"</span>: <span style="color: #569cd6;">true</span>,
            <span style="color: #ce9178;">"pipeline_write"</span>: <span style="color: #569cd6;">true</span>
        }
    }
}</code></pre>

            <table>
                <thead>
                    <tr>
                        <th>DeepSpeed ZeRO Stage</th>
                        <th>What's Offloaded to NVMe</th>
                        <th>Storage I/O Pattern</th>
                        <th>Bandwidth Needed</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>ZeRO-1</strong></td>
                        <td>Optimizer states only</td>
                        <td>Write-heavy during step</td>
                        <td class="medium">~1 GB/s</td>
                    </tr>
                    <tr>
                        <td><strong>ZeRO-2</strong></td>
                        <td>Optimizer + gradients</td>
                        <td>Bidirectional per step</td>
                        <td class="medium">~2-3 GB/s</td>
                    </tr>
                    <tr>
                        <td><strong>ZeRO-3</strong></td>
                        <td>Everything (params too)</td>
                        <td>Constant streaming</td>
                        <td class="poor">~5-10 GB/s</td>
                    </tr>
                    <tr>
                        <td><strong>ZeRO-Infinity</strong></td>
                        <td>CPU+NVMe unified memory</td>
                        <td>Demand paging</td>
                        <td class="poor">~10+ GB/s</td>
                    </tr>
                </tbody>
            </table>

            <h3 class="subsection-title">Megatron-LM Storage Patterns</h3>
            
            <p>Megatron-LM (NVIDIA's LLM training framework) has specific storage requirements:</p>

            <pre><code><span style="color: #6a9955;"># Megatron-LM data preprocessing and loading</span>
<span style="color: #6a9955;"># The key insight: Megatron uses memory-mapped binary files</span>

<span style="color: #6a9955;"># Step 1: Preprocess data into Megatron format</span>
python tools/preprocess_data.py \
    --input /data/corpus.json \
    --output-prefix /nvme/megatron_data/gpt2 \
    --vocab-file /models/gpt2-vocab.json \
    --dataset-impl mmap \           <span style="color: #6a9955;"># Memory-mapped for random access</span>
    --tokenizer-type GPT2BPETokenizer \
    --workers 64 \                  <span style="color: #6a9955;"># Parallel preprocessing</span>
    --append-eod

<span style="color: #6a9955;"># Step 2: Launch training with optimal I/O</span>
python pretrain_gpt.py \
    --data-path /nvme/megatron_data/gpt2 \
    --data-impl mmap \
    --num-workers 4 \               <span style="color: #6a9955;"># DataLoader workers</span>
    --checkpoint-activations \      <span style="color: #6a9955;"># Recompute vs store</span>
    --save /checkpoints \
    --save-interval 1000 \
    --async-save \                  <span style="color: #6a9955;"># Non-blocking checkpoints</span>
    --split 98,1,1                  <span style="color: #6a9955;"># Train/valid/test split</span></code></pre>

            <div class="info-box insight">
                <strong>ğŸ’¡ Megatron Storage Secret:</strong> Megatron's <code class="inline-code">mmap</code> implementation 
                is actually brilliant for NVMe. It lets the kernel's page cache and readahead do the buffering, which works 
                well for sequential training access. But for inference with random access patterns, you want 
                <code class="inline-code">O_DIRECT</code> to bypass the page cache.
            </div>

        </section>

        <!-- Section 2: Distributed Training Storage -->
        <section class="section" id="distributed-training">
            <h2 class="section-title">ğŸŒ Distributed Training Storage Patterns</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â Hard-Won Lesson:</strong> In 35 years, I've watched storage become the #1 bottleneck 
                as compute scaled. In the 90s, CPUs waited on disk. In the 2010s, we "fixed" it with SSDs. 
                Now with 8-GPU nodes doing 10 PFLOPS, storage is the bottleneck again. 
                The patterns below are battle-tested solutions.
            </div>

            <h3 class="subsection-title">Data Parallelism Storage Patterns</h3>
            
            <div class="architecture-diagram">
<span style="color: var(--accent-cyan);">Data Parallel Training - Storage Access Pattern</span>

Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â
Ã¢â€â€š                          SHARED STORAGE (NVMe-oF or Parallel FS)           Ã¢â€â€š
Ã¢â€â€š  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Shard 0    Ã¢â€â€š  Shard 1    Ã¢â€â€š  Shard 2    Ã¢â€â€š  Shard 3    Ã¢â€â€š  Shard N    Ã¢â€â€š   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  (GPU 0)    Ã¢â€â€š  (GPU 1)    Ã¢â€â€š  (GPU 2)    Ã¢â€â€š  (GPU 3)    Ã¢â€â€š  (GPU N)    Ã¢â€â€š   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â´Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â´Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â´Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â´Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ   Ã¢â€â€š
Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â€šÃ¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â€šÃ¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â€šÃ¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â€šÃ¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â€šÃ¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ
          Ã¢â€â€š             Ã¢â€â€š             Ã¢â€â€š             Ã¢â€â€š             Ã¢â€â€š
          Ã¢â€“Â¼             Ã¢â€“Â¼             Ã¢â€“Â¼             Ã¢â€“Â¼             Ã¢â€“Â¼
    Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â   Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â   Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â   Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â   Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â
    Ã¢â€â€š  GPU 0  Ã¢â€â€š   Ã¢â€â€š  GPU 1  Ã¢â€â€š   Ã¢â€â€š  GPU 2  Ã¢â€â€š   Ã¢â€â€š  GPU 3  Ã¢â€â€š   Ã¢â€â€š  GPU N  Ã¢â€â€š
    Ã¢â€â€š Worker  Ã¢â€â€š   Ã¢â€â€š Worker  Ã¢â€â€š   Ã¢â€â€š Worker  Ã¢â€â€š   Ã¢â€â€š Worker  Ã¢â€â€š   Ã¢â€â€š Worker  Ã¢â€â€š
    Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ
         Ã¢â€â€š             Ã¢â€â€š             Ã¢â€â€š             Ã¢â€â€š             Ã¢â€â€š
         Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬ AllReduce (NVLink/IB) Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ

<span style="color: var(--accent-yellow);">Key Storage Insight:</span> Each GPU reads DIFFERENT data (no overlap)
- Perfect for parallel file systems - no lock contention
- Bandwidth scales linearly with GPU count
- Challenge: Consistent sharding across epochs
            </div>

            <pre><code><span style="color: #6a9955;"># Data Parallel sharding with deterministic shuffling</span>
<span style="color: #c586c0;">import</span> torch.distributed <span style="color: #c586c0;">as</span> dist
<span style="color: #c586c0;">from</span> torch.utils.data <span style="color: #c586c0;">import</span> DistributedSampler

<span style="color: #c586c0;">class</span> <span style="color: #4ec9b0;">DeterministicDistributedSampler</span>(DistributedSampler):
    <span style="color: #ce9178;">"""Sampler that ensures consistent data sharding across restarts"""</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__init__</span>(self, dataset, num_replicas=<span style="color: #569cd6;">None</span>, rank=<span style="color: #569cd6;">None</span>, 
                 shuffle=<span style="color: #569cd6;">True</span>, seed=<span style="color: #b5cea8;">42</span>, epoch=<span style="color: #b5cea8;">0</span>):
        super().__init__(dataset, num_replicas, rank, shuffle, seed)
        self.epoch = epoch
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__iter__</span>(self):
        <span style="color: #6a9955;"># Deterministic shuffle based on epoch + seed</span>
        g = torch.Generator()
        g.manual_seed(self.seed + self.epoch)
        
        <span style="color: #c586c0;">if</span> self.shuffle:
            indices = torch.randperm(len(self.dataset), generator=g).tolist()
        <span style="color: #c586c0;">else</span>:
            indices = list(range(len(self.dataset)))
        
        <span style="color: #6a9955;"># Shard: each rank gets every Nth sample</span>
        indices = indices[self.rank:len(indices):self.num_replicas]
        
        <span style="color: #c586c0;">return</span> iter(indices)

<span style="color: #6a9955;"># Storage layout for optimal data parallel access</span>
<span style="color: #ce9178;">"""
Recommended directory structure:

/data/
  Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ shard_manifest.json      # Maps file Ã¢â€ â€™ shard assignment
  Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ epoch_0/
  Ã¢â€â€š   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ rank_0/
  Ã¢â€â€š   Ã¢â€â€š   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ batch_0000.bin   # Pre-sharded for rank 0
  Ã¢â€â€š   Ã¢â€â€š   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ batch_0001.bin
  Ã¢â€â€š   Ã¢â€â€š   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬ ...
  Ã¢â€â€š   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ rank_1/
  Ã¢â€â€š   Ã¢â€â€š   Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬ batch_0000.bin
  Ã¢â€â€š   Ã¢â€â€š   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬ ...
  Ã¢â€â€š   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬ ...
  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬ epoch_1/
      Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬ ... (reshuffled)

Benefits:
- No cross-rank I/O contention
- Predictable sequential access per rank
- Easy to prefetch next epoch while training current
"""</span></code></pre>

            <h3 class="subsection-title">Tensor Parallelism Storage Implications</h3>

            <div class="architecture-diagram">
<span style="color: var(--accent-cyan);">Tensor Parallel - Checkpoint Storage Challenge</span>

                    Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â
                    Ã¢â€â€š          Full Model Layer         Ã¢â€â€š
                    Ã¢â€â€š    Weight Matrix: [H Ã— 4H]        Ã¢â€â€š
                    Ã¢â€â€š    e.g., 12288 Ã— 49152 = 2.4GB    Ã¢â€â€š
                    Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ
                                    Ã¢â€â€š
              Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¼Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â
              Ã¢â€“Â¼                     Ã¢â€“Â¼                     Ã¢â€“Â¼
   Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â
   Ã¢â€â€š   GPU 0 Shard    Ã¢â€â€š  Ã¢â€â€š   GPU 1 Shard    Ã¢â€â€š  Ã¢â€â€š   GPU 2 Shard    Ã¢â€â€š
   Ã¢â€â€š  [H Ã— 4H/TP]     Ã¢â€â€š  Ã¢â€â€š  [H Ã— 4H/TP]     Ã¢â€â€š  Ã¢â€â€š  [H Ã— 4H/TP]     Ã¢â€â€š
   Ã¢â€â€š   12288 Ã— 16384  Ã¢â€â€š  Ã¢â€â€š   12288 Ã— 16384  Ã¢â€â€š  Ã¢â€â€š   12288 Ã— 16384  Ã¢â€â€š
   Ã¢â€â€š     ~800 MB      Ã¢â€â€š  Ã¢â€â€š     ~800 MB      Ã¢â€â€š  Ã¢â€â€š     ~800 MB      Ã¢â€â€š
   Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ

<span style="color: var(--accent-red);">Checkpoint Challenge:</span>
- Save sharded? Fast but requires same TP degree to load
- Save consolidated? Slow (all-gather) but portable
- Hybrid: Save sharded + async consolidation job
            </div>

            <pre><code><span style="color: #6a9955;"># Tensor Parallel checkpoint strategies</span>

<span style="color: #c586c0;">class</span> <span style="color: #4ec9b0;">TensorParallelCheckpointer</span>:
    <span style="color: #ce9178;">"""Smart checkpointing for tensor parallel models"""</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__init__</span>(self, tp_degree, checkpoint_dir):
        self.tp_degree = tp_degree
        self.tp_rank = dist.get_rank() % tp_degree
        self.checkpoint_dir = checkpoint_dir
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">save_sharded</span>(self, model, step):
        <span style="color: #ce9178;">"""Fast: Each TP rank saves its shard independently"""</span>
        shard_path = f<span style="color: #ce9178;">"{self.checkpoint_dir}/step_{step}/tp_rank_{self.tp_rank}.pt"</span>
        
        <span style="color: #6a9955;"># Only save this rank's parameters</span>
        local_state = {
            k: v <span style="color: #c586c0;">for</span> k, v <span style="color: #c586c0;">in</span> model.state_dict().items()
            <span style="color: #c586c0;">if</span> self._is_local_param(k)
        }
        
        <span style="color: #6a9955;"># Async save with background thread</span>
        torch.save(local_state, shard_path)
        
        <span style="color: #6a9955;"># Save metadata on rank 0</span>
        <span style="color: #c586c0;">if</span> self.tp_rank == <span style="color: #b5cea8;">0</span>:
            self._save_metadata(step)
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">save_consolidated</span>(self, model, step):
        <span style="color: #ce9178;">"""Slow but portable: Gather and save full model"""</span>
        
        full_state = {}
        <span style="color: #c586c0;">for</span> name, param <span style="color: #c586c0;">in</span> model.named_parameters():
            <span style="color: #c586c0;">if</span> self._is_tp_sharded(name):
                <span style="color: #6a9955;"># All-gather the shards</span>
                gathered = [torch.empty_like(param) <span style="color: #c586c0;">for</span> _ <span style="color: #c586c0;">in</span> range(self.tp_degree)]
                dist.all_gather(gathered, param)
                full_state[name] = torch.cat(gathered, dim=self._shard_dim(name))
            <span style="color: #c586c0;">else</span>:
                full_state[name] = param
        
        <span style="color: #6a9955;"># Only rank 0 saves</span>
        <span style="color: #c586c0;">if</span> self.tp_rank == <span style="color: #b5cea8;">0</span>:
            torch.save(full_state, f<span style="color: #ce9178;">"{self.checkpoint_dir}/step_{step}/consolidated.pt"</span>)</code></pre>

            <h3 class="subsection-title">Pipeline Parallelism Checkpoint Patterns</h3>
            
            <pre><code><span style="color: #6a9955;"># Pipeline Parallel checkpoint coordination</span>

<span style="color: #c586c0;">class</span> <span style="color: #4ec9b0;">PipelineCheckpointer</span>:
    <span style="color: #ce9178;">"""
    Pipeline parallel has unique checkpoint challenges:
    - Each stage has different layers
    - Must checkpoint at pipeline bubble (between microbatches)
    - Activations can be huge if not using recomputation
    """</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__init__</span>(self, pp_degree, stage_id):
        self.pp_degree = pp_degree
        self.stage_id = stage_id
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">save_stage</span>(self, model, optimizer, step, 
                    save_activations=<span style="color: #569cd6;">False</span>):
        <span style="color: #ce9178;">"""Save this pipeline stage"""</span>
        
        checkpoint = {
            <span style="color: #ce9178;">'stage_id'</span>: self.stage_id,
            <span style="color: #ce9178;">'model_state'</span>: model.state_dict(),
            <span style="color: #ce9178;">'optimizer_state'</span>: optimizer.state_dict(),
            <span style="color: #ce9178;">'step'</span>: step,
        }
        
        <span style="color: #c586c0;">if</span> save_activations:
            <span style="color: #6a9955;"># WARNING: This can be massive for large batch sizes</span>
            <span style="color: #6a9955;"># 70B model, batch=2048: ~500GB of activations</span>
            checkpoint[<span style="color: #ce9178;">'activations'</span>] = self._capture_activations()
        
        <span style="color: #6a9955;"># Stagger saves to avoid storage contention</span>
        <span style="color: #c586c0;">if</span> self.stage_id > <span style="color: #b5cea8;">0</span>:
            dist.barrier()  <span style="color: #6a9955;"># Wait for previous stage</span>
        
        torch.save(checkpoint, 
                   f<span style="color: #ce9178;">"checkpoint/step_{step}/stage_{self.stage_id}.pt"</span>)
        
        <span style="color: #c586c0;">if</span> self.stage_id < self.pp_degree - <span style="color: #b5cea8;">1</span>:
            dist.barrier()  <span style="color: #6a9955;"># Let next stage go</span>

<span style="color: #6a9955;"># Checkpoint frequency guidance</span>
CHECKPOINT_FREQUENCY_GUIDE = {
    <span style="color: #ce9178;">'model_size'</span>: {
        <span style="color: #ce9178;">'< 1B params'</span>: {
            <span style="color: #ce9178;">'interval'</span>: <span style="color: #ce9178;">'Every 1000 steps'</span>,
            <span style="color: #ce9178;">'reason'</span>: <span style="color: #ce9178;">'Small enough to checkpoint frequently'</span>
        },
        <span style="color: #ce9178;">'1-10B params'</span>: {
            <span style="color: #ce9178;">'interval'</span>: <span style="color: #ce9178;">'Every 500-1000 steps'</span>,
            <span style="color: #ce9178;">'reason'</span>: <span style="color: #ce9178;">'~20-200GB checkpoints, 1-5 min save time'</span>
        },
        <span style="color: #ce9178;">'10-100B params'</span>: {
            <span style="color: #ce9178;">'interval'</span>: <span style="color: #ce9178;">'Every 200-500 steps'</span>,
            <span style="color: #ce9178;">'reason'</span>: <span style="color: #ce9178;">'~200GB-2TB, async save mandatory'</span>
        },
        <span style="color: #ce9178;">'> 100B params'</span>: {
            <span style="color: #ce9178;">'interval'</span>: <span style="color: #ce9178;">'Every 100-200 steps'</span>,
            <span style="color: #ce9178;">'reason'</span>: <span style="color: #ce9178;">'Multi-TB, distributed save across nodes'</span>
        }
    },
    <span style="color: #ce9178;">'rule_of_thumb'</span>: <span style="color: #ce9178;">'Checkpoint often enough that restart cost < 10% of total training'</span>
}</code></pre>

            <h3 class="subsection-title">Model Sharding Storage Strategies</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Sharding Strategy</th>
                        <th>Storage Layout</th>
                        <th>Checkpoint Size</th>
                        <th>Load Time</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>FSDP Full State</strong></td>
                        <td>Single consolidated file</td>
                        <td class="good">1Ã— model size</td>
                        <td class="poor">Slow (gather required)</td>
                        <td>Inference export</td>
                    </tr>
                    <tr>
                        <td><strong>FSDP Sharded State</strong></td>
                        <td>One file per rank</td>
                        <td class="good">1Ã— model size total</td>
                        <td class="good">Fast parallel load</td>
                        <td>Training resume</td>
                    </tr>
                    <tr>
                        <td><strong>DeepSpeed Universal</strong></td>
                        <td>Sharded + metadata</td>
                        <td class="medium">~1.1Ã— (metadata overhead)</td>
                        <td class="good">Any topology load</td>
                        <td>Cross-cluster migration</td>
                    </tr>
                    <tr>
                        <td><strong>Megatron Distributed</strong></td>
                        <td>TPÃ—PP files</td>
                        <td class="good">1Ã— model size</td>
                        <td class="medium">Requires same topology</td>
                        <td>Large-scale training</td>
                    </tr>
                    <tr>
                        <td><strong>Safetensors</strong></td>
                        <td>Sharded safetensors</td>
                        <td class="good">1Ã— model size</td>
                        <td class="good">Memory-mapped load</td>
                        <td>HuggingFace ecosystem</td>
                    </tr>
                </tbody>
            </table>

        </section>

        <!-- Section 3: Production Monitoring Stack -->
        <section class="section" id="monitoring">
            <h2 class="section-title">ğŸ“Š Production Monitoring Stack</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â Monitoring Truth:</strong> In 35 years, every production outage I've seen could have been 
                prevented by proper monitoring. The dashboards below are what I wish I had in 1995 when 
                explaining to executives why their RAID array died. Today's GPU-storage systems are more complex 
                but the principle is the same: measure everything, alert early.
            </div>

            <h3 class="subsection-title">Prometheus Metrics Configuration</h3>
            
            <pre><code><span style="color: #6a9955;"># prometheus/gpu_storage_rules.yml</span>
groups:
  - name: gpu_storage_alerts
    interval: 15s
    rules:
      <span style="color: #6a9955;"># NVMe Health Alerts</span>
      - alert: NVMeHighLatency
        expr: nvme_read_latency_p99_us > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: <span style="color: #ce9178;">"NVMe P99 read latency > 500Âµs"</span>
          description: <span style="color: #ce9178;">"Drive {{ $labels.device }} showing high latency. Check for thermal throttling or wear."</span>
      
      - alert: NVMeCriticalWear
        expr: nvme_percentage_used > 90
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: <span style="color: #ce9178;">"NVMe drive > 90% lifetime wear"</span>
          description: <span style="color: #ce9178;">"{{ $labels.device }} at {{ $value }}% wear. Plan replacement within 30 days."</span>
      
      - alert: NVMeAvailableSparelow
        expr: nvme_available_spare_percent < 10
        labels:
          severity: critical
        annotations:
          summary: <span style="color: #ce9178;">"NVMe spare capacity critically low"</span>
      
      <span style="color: #6a9955;"># GPU-Storage Bandwidth Alerts</span>
      - alert: GDSBandwidthDegraded
        expr: rate(gds_bytes_read_total[5m]) < 5e9  <span style="color: #6a9955;"># < 5 GB/s</span>
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: <span style="color: #ce9178;">"GDS bandwidth below expected threshold"</span>
          description: <span style="color: #ce9178;">"GPU {{ $labels.gpu_id }} GDS throughput at {{ $value | humanize }}B/s"</span>
      
      - alert: PCIeBandwidthBottleneck
        expr: pcie_bandwidth_utilization > 0.85
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: <span style="color: #ce9178;">"PCIe bandwidth > 85% utilized"</span>
      
      <span style="color: #6a9955;"># Training Job Alerts</span>
      - alert: CheckpointWriteSlow
        expr: histogram_quantile(0.99, checkpoint_write_seconds_bucket) > 300
        labels:
          severity: warning
        annotations:
          summary: <span style="color: #ce9178;">"Checkpoint writes taking > 5 minutes"</span>
      
      - alert: DataLoadStall
        expr: rate(dataloader_samples_total[1m]) == 0 
               AND on(job) training_step_in_progress == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: <span style="color: #ce9178;">"Data loading stalled - GPUs likely idle"</span>

<span style="color: #6a9955;"># Recording rules for dashboard efficiency</span>
  - name: gpu_storage_recording
    rules:
      - record: job:nvme_iops:rate5m
        expr: sum(rate(nvme_read_commands_total[5m]) + rate(nvme_write_commands_total[5m])) by (device)
      
      - record: job:gds_bandwidth:rate5m
        expr: sum(rate(gds_bytes_read_total[5m]) + rate(gds_bytes_written_total[5m])) by (gpu_id)
      
      - record: job:storage_efficiency:ratio
        expr: |
          sum(rate(gds_bytes_read_total[5m])) / 
          sum(rate(nvme_bytes_read_total[5m]))</code></pre>

            <h3 class="subsection-title">Custom Metrics Exporter</h3>
            
            <pre><code><span style="color: #6a9955;"># gpu_storage_exporter.py - Custom Prometheus exporter</span>
<span style="color: #c586c0;">from</span> prometheus_client <span style="color: #c586c0;">import</span> start_http_server, Gauge, Counter, Histogram
<span style="color: #c586c0;">import</span> subprocess
<span style="color: #c586c0;">import</span> json
<span style="color: #c586c0;">import</span> time

<span style="color: #6a9955;"># NVMe metrics</span>
nvme_read_latency = Histogram(
    <span style="color: #ce9178;">'nvme_read_latency_seconds'</span>,
    <span style="color: #ce9178;">'NVMe read latency distribution'</span>,
    [<span style="color: #ce9178;">'device'</span>],
    buckets=[.0001, .0005, .001, .005, .01, .05, .1, .5, 1]
)

nvme_percentage_used = Gauge(
    <span style="color: #ce9178;">'nvme_percentage_used'</span>,
    <span style="color: #ce9178;">'NVMe lifetime percentage used'</span>,
    [<span style="color: #ce9178;">'device'</span>, <span style="color: #ce9178;">'serial'</span>]
)

nvme_temperature = Gauge(
    <span style="color: #ce9178;">'nvme_temperature_celsius'</span>,
    <span style="color: #ce9178;">'NVMe temperature'</span>,
    [<span style="color: #ce9178;">'device'</span>, <span style="color: #ce9178;">'sensor'</span>]
)

nvme_available_spare = Gauge(
    <span style="color: #ce9178;">'nvme_available_spare_percent'</span>,
    <span style="color: #ce9178;">'NVMe available spare capacity'</span>,
    [<span style="color: #ce9178;">'device'</span>]
)

<span style="color: #6a9955;"># GDS metrics</span>
gds_bytes_read = Counter(
    <span style="color: #ce9178;">'gds_bytes_read_total'</span>,
    <span style="color: #ce9178;">'Total bytes read via GDS'</span>,
    [<span style="color: #ce9178;">'gpu_id'</span>]
)

gds_read_latency = Histogram(
    <span style="color: #ce9178;">'gds_read_latency_seconds'</span>,
    <span style="color: #ce9178;">'GDS read latency'</span>,
    [<span style="color: #ce9178;">'gpu_id'</span>, <span style="color: #ce9178;">'operation_size'</span>],
    buckets=[.0001, .0005, .001, .002, .005, .01, .02, .05, .1]
)

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">collect_nvme_metrics</span>():
    <span style="color: #ce9178;">"""Collect metrics from nvme-cli"""</span>
    result = subprocess.run(
        [<span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'list'</span>, <span style="color: #ce9178;">'-o'</span>, <span style="color: #ce9178;">'json'</span>],
        capture_output=<span style="color: #569cd6;">True</span>, text=<span style="color: #569cd6;">True</span>
    )
    devices = json.loads(result.stdout)[<span style="color: #ce9178;">'Devices'</span>]
    
    <span style="color: #c586c0;">for</span> dev <span style="color: #c586c0;">in</span> devices:
        device_path = dev[<span style="color: #ce9178;">'DevicePath'</span>]
        
        <span style="color: #6a9955;"># Get SMART data</span>
        smart = subprocess.run(
            [<span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'smart-log'</span>, device_path, <span style="color: #ce9178;">'-o'</span>, <span style="color: #ce9178;">'json'</span>],
            capture_output=<span style="color: #569cd6;">True</span>, text=<span style="color: #569cd6;">True</span>
        )
        smart_data = json.loads(smart.stdout)
        
        nvme_percentage_used.labels(
            device=device_path,
            serial=dev[<span style="color: #ce9178;">'SerialNumber'</span>]
        ).set(smart_data[<span style="color: #ce9178;">'percent_used'</span>])
        
        nvme_temperature.labels(
            device=device_path,
            sensor=<span style="color: #ce9178;">'composite'</span>
        ).set(smart_data[<span style="color: #ce9178;">'temperature'</span>] - <span style="color: #b5cea8;">273</span>)  <span style="color: #6a9955;"># Kelvin to Celsius</span>
        
        nvme_available_spare.labels(
            device=device_path
        ).set(smart_data[<span style="color: #ce9178;">'avail_spare'</span>])

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">collect_dcgm_metrics</span>():
    <span style="color: #ce9178;">"""Collect GPU metrics from DCGM"""</span>
    <span style="color: #c586c0;">import</span> pydcgm
    <span style="color: #c586c0;">import</span> dcgm_fields
    
    dcgm_handle = pydcgm.DcgmHandle()
    group = dcgm_handle.GetDefaultGroup()
    
    <span style="color: #6a9955;"># Collect PCIe throughput (indicates storage traffic)</span>
    field_ids = [
        dcgm_fields.DCGM_FI_DEV_PCIE_TX_THROUGHPUT,
        dcgm_fields.DCGM_FI_DEV_PCIE_RX_THROUGHPUT,
        dcgm_fields.DCGM_FI_DEV_GPU_UTIL,
        dcgm_fields.DCGM_FI_DEV_MEM_COPY_UTIL,
    ]
    
    values = group.samples.GetLatest(field_ids)
    <span style="color: #6a9955;"># ... export to Prometheus gauges</span>

<span style="color: #c586c0;">if</span> __name__ == <span style="color: #ce9178;">'__main__'</span>:
    start_http_server(<span style="color: #b5cea8;">9090</span>)
    <span style="color: #c586c0;">while</span> <span style="color: #569cd6;">True</span>:
        collect_nvme_metrics()
        collect_dcgm_metrics()
        time.sleep(<span style="color: #b5cea8;">15</span>)</code></pre>

            <h3 class="subsection-title">Grafana Dashboard Configuration</h3>
            
            <pre><code><span style="color: #6a9955;"># grafana/dashboards/gpu_storage_overview.json (key panels)</span>
{
  <span style="color: #ce9178;">"title"</span>: <span style="color: #ce9178;">"GPU-Storage Performance Overview"</span>,
  <span style="color: #ce9178;">"panels"</span>: [
    {
      <span style="color: #ce9178;">"title"</span>: <span style="color: #ce9178;">"GDS Bandwidth by GPU"</span>,
      <span style="color: #ce9178;">"type"</span>: <span style="color: #ce9178;">"timeseries"</span>,
      <span style="color: #ce9178;">"targets"</span>: [{
        <span style="color: #ce9178;">"expr"</span>: <span style="color: #ce9178;">"sum(rate(gds_bytes_read_total[5m])) by (gpu_id)"</span>,
        <span style="color: #ce9178;">"legendFormat"</span>: <span style="color: #ce9178;">"GPU {{ gpu_id }}"</span>
      }],
      <span style="color: #ce9178;">"fieldConfig"</span>: {
        <span style="color: #ce9178;">"defaults"</span>: {
          <span style="color: #ce9178;">"unit"</span>: <span style="color: #ce9178;">"Bps"</span>,
          <span style="color: #ce9178;">"thresholds"</span>: {
            <span style="color: #ce9178;">"steps"</span>: [
              {<span style="color: #ce9178;">"value"</span>: <span style="color: #b5cea8;">0</span>, <span style="color: #ce9178;">"color"</span>: <span style="color: #ce9178;">"red"</span>},
              {<span style="color: #ce9178;">"value"</span>: <span style="color: #b5cea8;">3e9</span>, <span style="color: #ce9178;">"color"</span>: <span style="color: #ce9178;">"yellow"</span>},
              {<span style="color: #ce9178;">"value"</span>: <span style="color: #b5cea8;">5e9</span>, <span style="color: #ce9178;">"color"</span>: <span style="color: #ce9178;">"green"</span>}
            ]
          }
        }
      }
    },
    {
      <span style="color: #ce9178;">"title"</span>: <span style="color: #ce9178;">"NVMe Latency Heatmap"</span>,
      <span style="color: #ce9178;">"type"</span>: <span style="color: #ce9178;">"heatmap"</span>,
      <span style="color: #ce9178;">"targets"</span>: [{
        <span style="color: #ce9178;">"expr"</span>: <span style="color: #ce9178;">"sum(rate(nvme_read_latency_seconds_bucket[1m])) by (le)"</span>,
        <span style="color: #ce9178;">"format"</span>: <span style="color: #ce9178;">"heatmap"</span>
      }]
    },
    {
      <span style="color: #ce9178;">"title"</span>: <span style="color: #ce9178;">"Storage Efficiency Ratio"</span>,
      <span style="color: #ce9178;">"type"</span>: <span style="color: #ce9178;">"gauge"</span>,
      <span style="color: #ce9178;">"description"</span>: <span style="color: #ce9178;">"GDS bytes / Total NVMe bytes (higher = more direct GPU access)"</span>,
      <span style="color: #ce9178;">"targets"</span>: [{
        <span style="color: #ce9178;">"expr"</span>: <span style="color: #ce9178;">"job:storage_efficiency:ratio"</span>
      }],
      <span style="color: #ce9178;">"fieldConfig"</span>: {
        <span style="color: #ce9178;">"defaults"</span>: {
          <span style="color: #ce9178;">"min"</span>: <span style="color: #b5cea8;">0</span>,
          <span style="color: #ce9178;">"max"</span>: <span style="color: #b5cea8;">1</span>,
          <span style="color: #ce9178;">"thresholds"</span>: {
            <span style="color: #ce9178;">"steps"</span>: [
              {<span style="color: #ce9178;">"value"</span>: <span style="color: #b5cea8;">0</span>, <span style="color: #ce9178;">"color"</span>: <span style="color: #ce9178;">"red"</span>},
              {<span style="color: #ce9178;">"value"</span>: <span style="color: #b5cea8;">0.5</span>, <span style="color: #ce9178;">"color"</span>: <span style="color: #ce9178;">"yellow"</span>},
              {<span style="color: #ce9178;">"value"</span>: <span style="color: #b5cea8;">0.8</span>, <span style="color: #ce9178;">"color"</span>: <span style="color: #ce9178;">"green"</span>}
            ]
          }
        }
      }
    },
    {
      <span style="color: #ce9178;">"title"</span>: <span style="color: #ce9178;">"NVMe Drive Health"</span>,
      <span style="color: #ce9178;">"type"</span>: <span style="color: #ce9178;">"table"</span>,
      <span style="color: #ce9178;">"targets"</span>: [
        {<span style="color: #ce9178;">"expr"</span>: <span style="color: #ce9178;">"nvme_percentage_used"</span>, <span style="color: #ce9178;">"legendFormat"</span>: <span style="color: #ce9178;">"% Used"</span>},
        {<span style="color: #ce9178;">"expr"</span>: <span style="color: #ce9178;">"nvme_available_spare_percent"</span>, <span style="color: #ce9178;">"legendFormat"</span>: <span style="color: #ce9178;">"Spare %"</span>},
        {<span style="color: #ce9178;">"expr"</span>: <span style="color: #ce9178;">"nvme_temperature_celsius"</span>, <span style="color: #ce9178;">"legendFormat"</span>: <span style="color: #ce9178;">"Temp Ã‚Â°C"</span>}
      ]
    }
  ]
}</code></pre>

            <h3 class="subsection-title">DCGM (NVIDIA Data Center GPU Manager)</h3>
            
            <pre><code><span style="color: #6a9955;"># DCGM setup for GPU-storage correlation monitoring</span>

<span style="color: #6a9955;"># Install DCGM</span>
sudo apt-get install datacenter-gpu-manager

<span style="color: #6a9955;"># Start DCGM daemon</span>
sudo systemctl enable nvidia-dcgm
sudo systemctl start nvidia-dcgm

<span style="color: #6a9955;"># Create field group for storage-related metrics</span>
dcgmi group -c storage_monitoring
dcgmi group -g 1 -a 0,1,2,3,4,5,6,7  <span style="color: #6a9955;"># Add GPUs 0-7</span>

<span style="color: #6a9955;"># Key fields for storage correlation</span>
<span style="color: #6a9955;"># DCGM_FI_DEV_PCIE_TX_THROUGHPUT - PCIe TX (GPU Ã¢â€ â€™ storage for writes)</span>
<span style="color: #6a9955;"># DCGM_FI_DEV_PCIE_RX_THROUGHPUT - PCIe RX (storage Ã¢â€ â€™ GPU for reads)</span>
<span style="color: #6a9955;"># DCGM_FI_DEV_PCIE_REPLAY_COUNTER - PCIe errors (indicates link issues)</span>
<span style="color: #6a9955;"># DCGM_FI_DEV_GPU_UTIL - GPU utilization (low = data starvation)</span>
<span style="color: #6a9955;"># DCGM_FI_DEV_MEM_COPY_UTIL - Memory copy utilization</span>

<span style="color: #6a9955;"># Start Prometheus-compatible exporter</span>
dcgm-exporter --collectors /etc/dcgm-exporter/dcp-metrics-included.csv \
              --address :9400 \
              --collect-interval 5000  <span style="color: #6a9955;"># 5 second collection</span></code></pre>

            <h3 class="subsection-title">nvtop Real-Time Monitoring</h3>
            
            <pre><code><span style="color: #6a9955;"># nvtop installation and usage</span>

<span style="color: #6a9955;"># Install</span>
sudo apt-get install nvtop

<span style="color: #6a9955;"># Launch with storage-focused view</span>
nvtop --no-plot  <span style="color: #6a9955;"># Disable GPU plot to see more processes</span>

<span style="color: #6a9955;"># Key metrics to watch for storage issues:</span>
<span style="color: #6a9955;"># - MEM: If GPU memory is full, cannot prefetch data</span>
<span style="color: #6a9955;"># - GPU%: Low utilization often indicates I/O bottleneck</span>
<span style="color: #6a9955;"># - PCIe RX: Direct indicator of data flowing to GPU</span>

<span style="color: #6a9955;"># Programmatic monitoring with py3nvml</span>
<span style="color: #c586c0;">import</span> py3nvml.py3nvml <span style="color: #c586c0;">as</span> nvml

nvml.nvmlInit()
handle = nvml.nvmlDeviceGetHandleByIndex(<span style="color: #b5cea8;">0</span>)

<span style="color: #6a9955;"># Get PCIe throughput</span>
tx = nvml.nvmlDeviceGetPcieThroughput(handle, nvml.NVML_PCIE_UTIL_TX_BYTES)
rx = nvml.nvmlDeviceGetPcieThroughput(handle, nvml.NVML_PCIE_UTIL_RX_BYTES)

print(f<span style="color: #ce9178;">"PCIe TX: {tx / 1e9:.2f} GB/s"</span>)
print(f<span style="color: #ce9178;">"PCIe RX: {rx / 1e9:.2f} GB/s"</span>)

<span style="color: #6a9955;"># If RX is low during training Ã¢â€ â€™ storage bottleneck</span>
<span style="color: #6a9955;"># If GPU util is low + RX is low Ã¢â€ â€™ data loading issue</span></code></pre>

        </section>

        <!-- Section 3.5: Debugging & Tracing Tools -->
        <section class="section" id="debugging-tools">
            <h2 class="section-title">ğŸ” Debugging & Tracing Tools</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â Debug Like a Pro:</strong> When GPU training stalls and you don't know why, these tools 
                are your friends. I've diagnosed more storage issues with blktrace and bpftrace than with any 
                fancy monitoring dashboard. Low-level visibility wins.
            </div>

            <h3 class="subsection-title">nvme-cli Deep Dive Commands</h3>
            
            <pre><code><span style="color: #6a9955;"># Essential nvme-cli commands for GPU-storage debugging</span>

<span style="color: #6a9955;"># 1. Full device identification</span>
nvme id-ctrl /dev/nvme0 -H  <span style="color: #6a9955;"># Human-readable controller info</span>
<span style="color: #6a9955;"># Key fields: MDTS (max data transfer size), SGL support, CMB support</span>

<span style="color: #6a9955;"># 2. Namespace capabilities</span>
nvme id-ns /dev/nvme0n1 -H
<span style="color: #6a9955;"># Check: LBAF (LBA formats), metadata support, deallocation support</span>

<span style="color: #6a9955;"># 3. Latency histogram (if supported)</span>
nvme intel lat-stats /dev/nvme0 -w  <span style="color: #6a9955;"># Write latency histogram</span>
nvme intel lat-stats /dev/nvme0 -r  <span style="color: #6a9955;"># Read latency histogram</span>

<span style="color: #6a9955;"># 4. Internal temperature sensors</span>
nvme smart-log /dev/nvme0 -o json | jq '.temperature_sensor'

<span style="color: #6a9955;"># 5. Command effects log (what each command does)</span>
nvme effects-log /dev/nvme0

<span style="color: #6a9955;"># 6. Feature settings</span>
nvme get-feature /dev/nvme0 -f 0x01 -H  <span style="color: #6a9955;"># Arbitration</span>
nvme get-feature /dev/nvme0 -f 0x02 -H  <span style="color: #6a9955;"># Power Management</span>
nvme get-feature /dev/nvme0 -f 0x05 -H  <span style="color: #6a9955;"># Temperature Threshold</span>
nvme get-feature /dev/nvme0 -f 0x07 -H  <span style="color: #6a9955;"># Number of Queues</span>
nvme get-feature /dev/nvme0 -f 0x0c -H  <span style="color: #6a9955;"># APST (power states)</span>

<span style="color: #6a9955;"># 7. Error log entries</span>
nvme error-log /dev/nvme0 -e 64  <span style="color: #6a9955;"># Last 64 errors</span>

<span style="color: #6a9955;"># 8. Self-test (background)</span>
nvme device-self-test /dev/nvme0 -s 1  <span style="color: #6a9955;"># Short test</span>
nvme device-self-test /dev/nvme0 -s 2  <span style="color: #6a9955;"># Extended test</span>
nvme self-test-log /dev/nvme0         <span style="color: #6a9955;"># Check results</span></code></pre>

            <h3 class="subsection-title">blktrace for I/O Pattern Analysis</h3>
            
            <pre><code><span style="color: #6a9955;"># blktrace captures block-level I/O events</span>

<span style="color: #6a9955;"># Install</span>
sudo apt-get install blktrace

<span style="color: #6a9955;"># Capture trace during training (10 seconds)</span>
sudo blktrace -d /dev/nvme0n1 -w 10 -o trace

<span style="color: #6a9955;"># Analyze with blkparse</span>
blkparse -i trace -d trace.bin
<span style="color: #6a9955;"># Output shows: timestamp, CPU, action, RWBS, start_sector, size</span>

<span style="color: #6a9955;"># Quick summary with btt</span>
btt -i trace.bin -l trace.latency
<span style="color: #6a9955;"># Shows: Q2Q (queue to queue), D2C (dispatch to complete)</span>

<span style="color: #6a9955;"># Identify read/write patterns</span>
blkparse -i trace | awk '/R |W / {print $6, $7, $8}' | \
    sort -n | uniq -c | sort -rn | head -20
<span style="color: #6a9955;"># Shows most common I/O sizes and offsets</span>

<span style="color: #6a9955;"># Detect sequential vs random</span>
blkparse -i trace | awk '
    /R |W / {
        if (NR > 1 && $7 == prev_end) seq++;
        else rand++;
        prev_end = $7 + $8;
    }
    END { print "Sequential:", seq, "Random:", rand }'

<span style="color: #6a9955;"># Live monitoring (one-liner)</span>
sudo blktrace -d /dev/nvme0n1 -o - | blkparse -i - -o /dev/stdout | \
    awk '/R |W / {sum+=$8; count++} END {print "Avg I/O size:", sum/count*512, "bytes"}'</code></pre>

            <h3 class="subsection-title">bpftrace for Deep Kernel Analysis</h3>
            
            <pre><code><span style="color: #6a9955;"># bpftrace: Dynamic tracing for storage debugging</span>

<span style="color: #6a9955;"># Install</span>
sudo apt-get install bpftrace

<span style="color: #6a9955;"># 1. NVMe command latency distribution</span>
sudo bpftrace -e '
kprobe:nvme_queue_rq { @start[arg0] = nsecs; }
kretprobe:nvme_queue_rq /@start[arg0]/ {
    @latency_us = hist((nsecs - @start[arg0]) / 1000);
    delete(@start[arg0]);
}'

<span style="color: #6a9955;"># 2. Track large I/O requests (>1MB)</span>
sudo bpftrace -e '
tracepoint:block:block_rq_issue
/args->bytes > 1048576/ {
    printf("%s: %s %d bytes at %lld\n", 
           comm, args->rwbs, args->bytes, args->sector);
}'

<span style="color: #6a9955;"># 3. Find which process is doing I/O</span>
sudo bpftrace -e '
tracepoint:block:block_rq_complete {
    @io_by_proc[comm] = sum(args->nr_sector * 512);
}'

<span style="color: #6a9955;"># 4. Detect I/O stalls (>10ms latency)</span>
sudo bpftrace -e '
tracepoint:block:block_rq_issue { @start[args->sector] = nsecs; }
tracepoint:block:block_rq_complete /@start[args->sector]/ {
    $lat = (nsecs - @start[args->sector]) / 1000000;
    if ($lat > 10) {
        printf("SLOW I/O: %d ms, sector %lld\n", $lat, args->sector);
    }
    delete(@start[args->sector]);
}'

<span style="color: #6a9955;"># 5. GDS (cuFile) call tracing</span>
sudo bpftrace -e '
uprobe:/usr/lib/x86_64-linux-gnu/libcufile.so:cuFileRead {
    @reads = count();
    @read_start[tid] = nsecs;
}
uretprobe:/usr/lib/x86_64-linux-gnu/libcufile.so:cuFileRead /@read_start[tid]/ {
    @read_latency = hist((nsecs - @read_start[tid]) / 1000);
    delete(@read_start[tid]);
}'</code></pre>

            <h3 class="subsection-title">iostat for Quick Health Checks</h3>
            
            <pre><code><span style="color: #6a9955;"># iostat: Quick I/O performance overview</span>

<span style="color: #6a9955;"># Extended stats, 1-second intervals</span>
iostat -xz 1 /dev/nvme0n1
<span style="color: #6a9955;"># Key columns:</span>
<span style="color: #6a9955;"># r/s, w/s:     IOPS</span>
<span style="color: #6a9955;"># rMB/s, wMB/s: Throughput</span>
<span style="color: #6a9955;"># r_await, w_await: Latency in ms (should be <1 for NVMe)</span>
<span style="color: #6a9955;"># aqu-sz:       Queue depth</span>
<span style="color: #6a9955;"># %util:        Utilization (misleading for NVMe, ignore)</span>

<span style="color: #6a9955;"># Alert on high latency</span>
iostat -xz 1 | awk '/nvme/ && $10 > 5 {print "HIGH LATENCY:", $0}'

<span style="color: #6a9955;"># JSON output for programmatic parsing</span>
iostat -xzo JSON 1 1 | jq '.sysstat.hosts[0].statistics[0].disk'</code></pre>

            <h3 class="subsection-title">fio Diagnostic Workloads</h3>
            
            <pre><code><span style="color: #6a9955;"># fio: Generate controlled workloads for diagnosis</span>

<span style="color: #6a9955;"># 1. Baseline sequential read (should match SSD spec)</span>
fio --name=seq_read --filename=/dev/nvme0n1 --direct=1 \
    --rw=read --bs=128k --iodepth=32 --numjobs=4 \
    --runtime=30 --group_reporting

<span style="color: #6a9955;"># 2. 4K random read (tests IOPS)</span>
fio --name=rand_read --filename=/dev/nvme0n1 --direct=1 \
    --rw=randread --bs=4k --iodepth=256 --numjobs=4 \
    --runtime=30 --group_reporting

<span style="color: #6a9955;"># 3. Checkpoint simulation (large sequential writes)</span>
fio --name=checkpoint --filename=/mnt/nvme/test --direct=1 \
    --rw=write --bs=1m --iodepth=8 --numjobs=1 \
    --size=10G --runtime=60 --group_reporting

<span style="color: #6a9955;"># 4. Mixed training workload simulation</span>
fio --name=training --filename=/mnt/nvme/test --direct=1 \
    --rw=randrw --rwmixread=90 --bs=64k --iodepth=16 \
    --numjobs=8 --runtime=60 --group_reporting \
    --lat_percentiles=1 --percentile_list=50:90:99:99.9

<span style="color: #6a9955;"># 5. GDS-like I/O pattern (io_uring, large blocks)</span>
fio --name=gds_like --filename=/dev/nvme0n1 --direct=1 \
    --ioengine=io_uring --rw=read --bs=1m --iodepth=64 \
    --numjobs=4 --runtime=30 --group_reporting</code></pre>

            <h4 style="color: #00ff88; margin: 25px 0 15px 0;">Real fio Output: What Good Looks Like</h4>
            
            <p>Here's actual output from a healthy enterprise NVMe SSD (Samsung PM9A3 7.68TB) - use these as baselines:</p>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># Sequential Read Baseline (should see 6.5+ GB/s)</span>
<span style="color: #6a9955;"># ============================================</span>
$ fio --name=seq_read --filename=/dev/nvme0n1 --direct=1 \
    --rw=read --bs=128k --iodepth=32 --numjobs=4 --runtime=30

seq_read: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB
  cpu          : usr=2.31%, sys=18.42%, ctx=1623847, majf=0, minf=524
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=99.8%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=1594832,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=<span style="color: #4ec9b0;">6847MiB/s</span> (7179MB/s), 1711MiB/s-1712MiB/s (1795MB/s-1795MB/s), io=<span style="color: #4ec9b0;">200GiB</span> (215GB), run=30001-30001msec

<span style="color: #6a9955;"># âœ“ GOOD: 6.8 GB/s sequential read (close to spec 7.0 GB/s)</span>
<span style="color: #6a9955;"># âœ— BAD:  &lt;5 GB/s would indicate thermal throttling or link issues</span></code></pre>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># 4K Random Read IOPS (should see 1M+ IOPS)</span>
<span style="color: #6a9955;"># ============================================</span>
$ fio --name=rand_read --filename=/dev/nvme0n1 --direct=1 \
    --rw=randread --bs=4k --iodepth=256 --numjobs=4 --runtime=30 \
    --lat_percentiles=1

rand_read: (groupid=0, jobs=4): err= 0: pid=12345: Sat Dec 21 10:30:45 2024
  read: IOPS=<span style="color: #4ec9b0;">1052k</span>, BW=4109MiB/s (4309MB/s)(120GiB/30001msec)
    slat (nsec): min=1203, max=89432, avg=2847.32, stdev=1023.11
    clat (usec): min=48, max=2847, avg=<span style="color: #4ec9b0;">92.31</span>, stdev=24.87
     lat (usec): min=51, max=2853, avg=95.16, stdev=25.12
    <span style="color: #4ec9b0;">clat percentiles (usec):</span>
     |  1.00th=[   62],  5.00th=[   68], 10.00th=[   72], 20.00th=[   77],
     | 30.00th=[   81], 40.00th=[   85], 50.00th=[   89], 60.00th=[   93],
     | 70.00th=[   98], 80.00th=[  105], 90.00th=[  118], 95.00th=[  133],
     | 99.00th=[  174], 99.50th=[  200], 99.90th=[  302], 99.95th=[  383],
     | 99.99th=[  783]
   bw (  MiB/s): min= 3987, max= 4234, per=100.00%, avg=4109.42, stdev=23.14
   iops        : min=1020832, max=1083904, avg=1052012.34, stdev=5923.41
  lat (usec)   : 50=0.01%, 100=72.31%, 250=27.52%, 500=0.15%, 750=0.01%
  lat (usec)   : 1000=0.01%

<span style="color: #6a9955;"># âœ“ GOOD: 1.05M IOPS, 92Âµs average latency, P99=174Âµs</span>
<span style="color: #6a9955;"># âœ— BAD:  &lt;800K IOPS or P99 &gt;500Âµs indicates issues</span></code></pre>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># Checkpoint Write Pattern (sustained writes)</span>
<span style="color: #6a9955;"># ============================================</span>
$ fio --name=checkpoint --filename=/mnt/nvme/ckpt_test --direct=1 \
    --rw=write --bs=1m --iodepth=8 --numjobs=4 --size=50G --runtime=60 \
    --lat_percentiles=1

checkpoint: (groupid=0, jobs=4): err= 0
  write: IOPS=4823, BW=<span style="color: #4ec9b0;">4823MiB/s</span> (5057MB/s)(200GiB/42457msec); 0 zone resets
    slat (usec): min=18, max=1234, avg=28.43, stdev=12.34
    clat (usec): min=312, max=18432, avg=<span style="color: #4ec9b0;">1623.21</span>, stdev=834.12
     lat (usec): min=342, max=18523, avg=1651.64, stdev=839.23
    <span style="color: #4ec9b0;">clat percentiles (usec):</span>
     |  1.00th=[  603],  5.00th=[  734], 10.00th=[  832], 20.00th=[ 1012],
     | 30.00th=[ 1172], 40.00th=[ 1336], 50.00th=[ 1500], 60.00th=[ 1680],
     | 70.00th=[ 1876], 80.00th=[ 2147], 90.00th=[ 2638], 95.00th=[ 3163],
     | 99.00th=[ 4621], <span style="color: #ff6b6b;">99.50th=[ 6587]</span>, 99.90th=[10945], 99.95th=[13698]

<span style="color: #6a9955;"># Ã¢Å¡Â Ã¯Â¸Â NOTE: Sustained write latency increases over time due to GC</span>
<span style="color: #6a9955;"># âœ“ GOOD: 4.8 GB/s sustained, P99 ~4.6ms</span>
<span style="color: #6a9955;"># âœ— BAD:  P99 &gt;20ms indicates GC pressure - check WAF!</span></code></pre>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># Mixed Read/Write (AI Training Simulation)</span>
<span style="color: #6a9955;"># ============================================</span>
$ fio --name=training --filename=/mnt/nvme/train_test --direct=1 \
    --rw=randrw --rwmixread=90 --bs=64k --iodepth=16 --numjobs=8 \
    --runtime=60 --group_reporting --lat_percentiles=1

training: (groupid=0, jobs=8): err= 0
  read: IOPS=<span style="color: #4ec9b0;">48234</span>, BW=2949MiB/s (3092MB/s)(173GiB/60001msec)
    clat (usec): min=89, max=8934, avg=<span style="color: #4ec9b0;">245.32</span>, stdev=123.45
    clat percentiles (usec):
     | 50.00th=[  223], 90.00th=[  359], 95.00th=[  445], 99.00th=[  701]
  write: IOPS=<span style="color: #4ec9b0;">5359</span>, BW=327MiB/s (343MB/s)(19.2GiB/60001msec); 0 zone resets
    clat (usec): min=234, max=15234, avg=<span style="color: #4ec9b0;">834.21</span>, stdev=345.67
    clat percentiles (usec):
     | 50.00th=[  734], 90.00th=[ 1254], 95.00th=[ 1614], 99.00th=[ 2868]

<span style="color: #6a9955;"># âœ“ GOOD: Read ~3 GB/s, Write 327 MB/s (90/10 mix)</span>
<span style="color: #6a9955;"># âœ“ GOOD: Read latency P99 &lt;1ms during mixed workload</span>
<span style="color: #6a9955;"># Matches typical training batch loading pattern</span></code></pre>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># io_uring with GDS-like Pattern</span>
<span style="color: #6a9955;"># ============================================</span>
$ fio --name=gds_like --filename=/dev/nvme0n1 --direct=1 \
    --ioengine=io_uring --hipri=1 --sqthread_poll=1 \
    --rw=read --bs=1m --iodepth=64 --numjobs=4 --runtime=30

gds_like: (groupid=0, jobs=4): err= 0
  read: IOPS=6234, BW=<span style="color: #4ec9b0;">6234MiB/s</span> (6537MB/s)(182GiB/30001msec)
    slat (nsec): min=934, max=23421, avg=1823.12, stdev=423.34
    clat (usec): min=187, max=12345, avg=<span style="color: #4ec9b0;">398.23</span>, stdev=187.34
  cpu          : usr=1.23%, sys=8.45%, ctx=23456, majf=0, minf=1284
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.2%, 16=0.4%, 32=12.3%, >=64=86.9%

<span style="color: #6a9955;"># âœ“ GOOD: 6.2 GB/s with io_uring SQPOLL</span>
<span style="color: #6a9955;"># âœ“ GOOD: Low CPU overhead (sys=8.45%)</span>
<span style="color: #6a9955;"># âœ“ GOOD: Submission latency ~1.8Âµs (slat)</span>
<span style="color: #6a9955;"># This is close to what GDS achieves for pure NVMe</span></code></pre>

            <h4 style="color: #ff6b6b; margin: 25px 0 15px 0;">What Bad Output Looks Like (Red Flags)</h4>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># RED FLAG: Thermal Throttling</span>
<span style="color: #6a9955;"># ============================================</span>
$ fio --name=sustained_write --filename=/dev/nvme0n1 --direct=1 \
    --rw=write --bs=1m --iodepth=32 --numjobs=4 --runtime=300

<span style="color: #ff6b6b;"># Output showing throttling:</span>
  write: IOPS=2134, BW=<span style="color: #ff6b6b;">2134MiB/s</span> (2238MB/s)   <span style="color: #ff6b6b;"># Started at 5 GB/s!</span>
    clat (msec): min=2, max=<span style="color: #ff6b6b;">234</span>, avg=<span style="color: #ff6b6b;">58.23</span>  <span style="color: #ff6b6b;"># Huge latency spike!</span>
    clat percentiles (msec):
     | 99.00th=[<span style="color: #ff6b6b;">178</span>], 99.50th=[<span style="color: #ff6b6b;">201</span>]             <span style="color: #ff6b6b;"># P99 &gt;100ms is bad</span>

<span style="color: #ff6b6b;"># Check temperature during test:</span>
$ nvme smart-log /dev/nvme0 | grep -i temp
temperature                         : <span style="color: #ff6b6b;">78Ã‚Â°C</span>    <span style="color: #ff6b6b;"># Throttle threshold!</span>
Warning  Coverage Temperature Time  : 234      <span style="color: #ff6b6b;"># 234 minutes above warning</span>

<span style="color: #ff6b6b;"># FIX: Improve cooling, reduce write intensity, add thermal pads</span></code></pre>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># RED FLAG: PCIe Link Degradation</span>
<span style="color: #6a9955;"># ============================================</span>
$ fio --name=seq_read --filename=/dev/nvme0n1 --direct=1 \
    --rw=read --bs=128k --iodepth=32 --numjobs=4 --runtime=30

<span style="color: #ff6b6b;"># Suspiciously low bandwidth:</span>
   READ: bw=<span style="color: #ff6b6b;">1623MiB/s</span> (1702MB/s)   <span style="color: #ff6b6b;"># Should be 6+ GB/s!</span>

<span style="color: #ff6b6b;"># Check link status:</span>
$ lspci -vvv -s 01:00.0 | grep -E "LnkCap|LnkSta"
LnkCap: Port #0, Speed 16GT/s (Gen4), Width x4
LnkSta: Speed 16GT/s (ok), Width <span style="color: #ff6b6b;">x1</span> (downgraded from x4)   <span style="color: #ff6b6b;"># PROBLEM!</span>

<span style="color: #ff6b6b;"># FIX: Reseat SSD, check slot, replace riser card</span>
$ echo 1 > /sys/bus/pci/devices/0000:01:00.0/remove
$ echo 1 > /sys/bus/pci/rescan</code></pre>

            <div class="info-box">
                <strong>ğŸ’¡ Debug Decision Tree:</strong>
                <ul style="margin-top: 10px; margin-bottom: 0;">
                    <li><strong>High latency spikes?</strong> Ã¢â€ â€™ blktrace + btt for latency breakdown</li>
                    <li><strong>Low throughput?</strong> Ã¢â€ â€™ fio baseline to check SSD health</li>
                    <li><strong>Random vs sequential?</strong> Ã¢â€ â€™ blkparse pattern analysis</li>
                    <li><strong>Which process?</strong> Ã¢â€ â€™ bpftrace by-process I/O</li>
                    <li><strong>GDS not working?</strong> Ã¢â€ â€™ uprobe cuFile functions</li>
                    <li><strong>Power state issues?</strong> Ã¢â€ â€™ nvme get-feature 0x0c</li>
                </ul>
            </div>
        </section>

        <!-- Section 4: Virtualization & Multi-tenancy -->
        <section class="section" id="virtualization">
            <h2 class="section-title">Ã°Å¸â€â€™ Virtualization & Multi-tenancy</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â Virtualization Reality Check:</strong> I've been doing storage virtualization since VMware ESX 2.0. 
                The promise is typically "transparent performance." The reality is often "hidden overhead." 
                GPU passthrough adds another layer of complexity. Here's what actually works.
            </div>

            <h3 class="subsection-title">IOMMU Configuration for GPU Passthrough</h3>
            
            <p>IOMMU (Input-Output Memory Management Unit) is <strong>mandatory</strong> for secure GPU passthrough. 
            Without it, a VM's GPU could access any host memory â€” a massive security hole.</p>

            <pre><code><span style="color: #6a9955;"># Enable IOMMU in GRUB (Intel)</span>
<span style="color: #6a9955;"># /etc/default/grub</span>
GRUB_CMDLINE_LINUX="intel_iommu=on iommu=pt"

<span style="color: #6a9955;"># Enable IOMMU in GRUB (AMD)</span>
GRUB_CMDLINE_LINUX="amd_iommu=on iommu=pt"

<span style="color: #6a9955;"># Apply changes</span>
sudo update-grub
sudo reboot

<span style="color: #6a9955;"># Verify IOMMU is active</span>
dmesg | grep -i iommu
<span style="color: #6a9955;"># Should see: "DMAR: IOMMU enabled"</span>

<span style="color: #6a9955;"># List IOMMU groups (GPUs should be in separate groups for passthrough)</span>
<span style="color: #c586c0;">for</span> d <span style="color: #c586c0;">in</span> /sys/kernel/iommu_groups/*/devices/*; <span style="color: #c586c0;">do</span>
    n=$(echo $d | cut -d/ -f5)
    echo <span style="color: #ce9178;">"IOMMU Group $n:"</span>
    lspci -nns <span style="color: #ce9178;">"${d##*/}"</span>
<span style="color: #c586c0;">done</span>

<span style="color: #6a9955;"># Ideal output: Each GPU in its own IOMMU group</span>
<span style="color: #6a9955;"># IOMMU Group 13: 41:00.0 3D controller [0302]: NVIDIA H100 [10de:2330]</span>
<span style="color: #6a9955;"># IOMMU Group 14: 61:00.0 3D controller [0302]: NVIDIA H100 [10de:2330]</span>

<span style="color: #6a9955;"># If GPUs share IOMMU group with other devices Ã¢â€ â€™ ACS override needed</span>
<span style="color: #6a9955;"># WARNING: This reduces security isolation</span>
GRUB_CMDLINE_LINUX="intel_iommu=on pcie_acs_override=downstream,multifunction"</code></pre>

            <h3 class="subsection-title">VFIO Setup for GPU Passthrough</h3>
            
            <pre><code><span style="color: #6a9955;"># VFIO (Virtual Function I/O) configuration</span>
<span style="color: #6a9955;"># VFIO gives VMs direct access to PCIe devices</span>

<span style="color: #6a9955;"># 1. Identify GPU PCI IDs</span>
lspci -nn | grep -i nvidia
<span style="color: #6a9955;"># 41:00.0 3D controller [0302]: NVIDIA H100 [10de:2330] (rev a1)</span>
<span style="color: #6a9955;"># 41:00.1 Audio device [0403]: NVIDIA [10de:2331] (rev a1)</span>

<span style="color: #6a9955;"># 2. Bind GPU to VFIO driver (prevent host from using it)</span>
<span style="color: #6a9955;"># /etc/modprobe.d/vfio.conf</span>
options vfio-pci ids=10de:2330,10de:2331

<span style="color: #6a9955;"># 3. Load VFIO modules early</span>
<span style="color: #6a9955;"># /etc/modules-load.d/vfio.conf</span>
vfio
vfio_iommu_type1
vfio_pci

<span style="color: #6a9955;"># 4. Ensure VFIO loads before NVIDIA driver</span>
<span style="color: #6a9955;"># /etc/modprobe.d/vfio.conf</span>
softdep nvidia pre: vfio-pci
softdep nvidiafb pre: vfio-pci
softdep nvidia_drm pre: vfio-pci

<span style="color: #6a9955;"># 5. Update initramfs</span>
sudo update-initramfs -u

<span style="color: #6a9955;"># 6. Verify VFIO binding after reboot</span>
lspci -k -s 41:00.0
<span style="color: #6a9955;"># Kernel driver in use: vfio-pci</span></code></pre>

            <h3 class="subsection-title">SR-IOV for NVMe Virtualization</h3>
            
            <div class="info-box warning">
                <strong>Ã¢Å¡Â Ã¯Â¸Â SR-IOV NVMe Reality:</strong> SR-IOV (Single Root I/O Virtualization) for NVMe is still 
                emerging. Most enterprise SSDs don't support it yet. Samsung PM1735/PM1743 and some Intel Optane 
                drives have SR-IOV, but it's not universal. Plan accordingly.
            </div>

            <pre><code><span style="color: #6a9955;"># SR-IOV NVMe configuration (when supported)</span>

<span style="color: #6a9955;"># Check if NVMe controller supports SR-IOV</span>
lspci -v -s $(lspci | grep -i nvme | awk '{print $1}') | grep -i "SR-IOV"
<span style="color: #6a9955;"># Capabilities: ... SR-IOV</span>

<span style="color: #6a9955;"># Enable SR-IOV Virtual Functions</span>
<span style="color: #6a9955;"># Number of VFs depends on controller capability</span>
echo 4 > /sys/bus/pci/devices/0000:03:00.0/sriov_numvfs

<span style="color: #6a9955;"># Verify VFs created</span>
lspci | grep -i nvme
<span style="color: #6a9955;"># 03:00.0 Non-Volatile memory controller: Samsung PM1735 (PF)</span>
<span style="color: #6a9955;"># 03:00.1 Non-Volatile memory controller: Samsung PM1735 (VF)</span>
<span style="color: #6a9955;"># 03:00.2 Non-Volatile memory controller: Samsung PM1735 (VF)</span>
<span style="color: #6a9955;"># ...</span>

<span style="color: #6a9955;"># Bind VFs to VFIO for VM passthrough</span>
echo "0000:03:00.1" > /sys/bus/pci/drivers/nvme/unbind
echo "0000:03:00.1" > /sys/bus/pci/drivers/vfio-pci/bind

<span style="color: #6a9955;"># Configure NVMe namespace assignment per VF</span>
<span style="color: #6a9955;"># This requires nvme-cli with SR-IOV support</span>
nvme virt-mgmt /dev/nvme0 -c 1 -r 0 -n 1 -a 8  <span style="color: #6a9955;"># Assign NS 1 to VF 1</span>
nvme virt-mgmt /dev/nvme0 -c 2 -r 0 -n 2 -a 8  <span style="color: #6a9955;"># Assign NS 2 to VF 2</span></code></pre>

            <h3 class="subsection-title">PASID (Process Address Space ID)</h3>
            
            <p>PASID enables multiple process address spaces on a single PCIe device â€” critical for 
            shared GPU access and CXL.</p>

            <pre><code><span style="color: #6a9955;"># PASID configuration for advanced GPU sharing</span>

<span style="color: #6a9955;"># Check PASID support</span>
lspci -v -s $(lspci | grep -i nvidia | head -1 | awk '{print $1}') | grep -i pasid
<span style="color: #6a9955;"># Capabilities: ... PASID</span>

<span style="color: #6a9955;"># PASID enables:</span>
<span style="color: #6a9955;"># 1. Multiple processes sharing a GPU with isolated address spaces</span>
<span style="color: #6a9955;"># 2. SVM (Shared Virtual Memory) between CPU and GPU</span>
<span style="color: #6a9955;"># 3. CXL.mem with per-process isolation</span>

<span style="color: #6a9955;"># Enable PASID in kernel (usually automatic with IOMMU)</span>
<span style="color: #6a9955;"># Verify with:</span>
cat /sys/class/iommu/*/intel-iommu/cap
<span style="color: #6a9955;"># Look for PASID capability bits</span>

<span style="color: #6a9955;"># For CXL devices with PASID</span>
<span style="color: #6a9955;"># Each GPU process gets unique PASID</span>
<span style="color: #6a9955;"># Device uses PASID to route memory requests to correct process space</span></code></pre>

            <h3 class="subsection-title">Multi-Tenant GPU-Storage Architecture</h3>
            
            <div class="architecture-diagram">
<span style="color: var(--accent-cyan);">Multi-Tenant GPU-Storage Architecture</span>

Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â
Ã¢â€â€š                           Kubernetes Cluster                           Ã¢â€â€š
Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¤
Ã¢â€â€š                                                                        Ã¢â€â€š
Ã¢â€â€š  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š    Tenant A Pod     Ã¢â€â€š  Ã¢â€â€š    Tenant B Pod     Ã¢â€â€š  Ã¢â€â€š  Tenant C Pod  Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€â€š  Ã¢â€â€š Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š GPU 0 (VFIO)  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š GPU 2 (VFIO)  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š Ã¢â€â€šGPU 4 (MIG) Ã¢â€â€š Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š GPU 1 (VFIO)  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š GPU 3 (VFIO)  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š Ã¢â€â€š  2g.20gb   Ã¢â€â€š Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€š  Ã¢â€â€š Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š          Ã¢â€â€š          Ã¢â€â€š  Ã¢â€â€š          Ã¢â€â€š          Ã¢â€â€š  Ã¢â€â€š       Ã¢â€â€š        Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€“Â¼Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€“Â¼Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€â€š  Ã¢â€â€š Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€“Â¼Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€šNVMe VF 0,1    Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€šNVMe VF 2,3    Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š Ã¢â€â€šNVMe NS 5   Ã¢â€â€š Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š(SR-IOV)       Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š(SR-IOV)       Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š Ã¢â€â€š(Namespace) Ã¢â€â€š Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€š  Ã¢â€â€š Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ Ã¢â€â€š  Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€š
Ã¢â€â€š                                                                        Ã¢â€â€š
Ã¢â€Å“Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â¤
Ã¢â€â€š                         Physical Hardware                              Ã¢â€â€š
Ã¢â€â€š  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š                    NVMe SSDs with SR-IOV                       Ã¢â€â€š   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â  Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â       Ã¢â€â€š   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š  VF 0    Ã¢â€â€š  Ã¢â€â€š  VF 1    Ã¢â€â€š  Ã¢â€â€š  VF 2    Ã¢â€â€š  Ã¢â€â€š  VF 3    Ã¢â€â€š  ...  Ã¢â€â€š   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€š  1 TB    Ã¢â€â€š  Ã¢â€â€š  1 TB    Ã¢â€â€š  Ã¢â€â€š  1 TB    Ã¢â€â€š  Ã¢â€â€š  1 TB    Ã¢â€â€š       Ã¢â€â€š   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€š  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ       Ã¢â€â€š   Ã¢â€â€š
Ã¢â€â€š  Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ   Ã¢â€â€š
Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ

<span style="color: var(--accent-green);">Isolation Mechanisms:</span>
â€¢ IOMMU: Memory isolation between VMs/containers
â€¢ VFIO: Secure device passthrough
â€¢ SR-IOV: Hardware-level NVMe partitioning
â€¢ NVMe Namespaces: Logical storage isolation
â€¢ MIG (Multi-Instance GPU): GPU partitioning for smaller workloads
            </div>

            <table>
                <thead>
                    <tr>
                        <th>Isolation Method</th>
                        <th>Performance Overhead</th>
                        <th>Security Level</th>
                        <th>Flexibility</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>VFIO Passthrough</strong></td>
                        <td class="good">&lt;2%</td>
                        <td class="good">High (IOMMU)</td>
                        <td class="poor">Low (1:1 mapping)</td>
                        <td>Dedicated GPU per VM</td>
                    </tr>
                    <tr>
                        <td><strong>SR-IOV VFs</strong></td>
                        <td class="good">&lt;5%</td>
                        <td class="good">High (hardware)</td>
                        <td class="medium">Medium</td>
                        <td>NVMe multi-tenancy</td>
                    </tr>
                    <tr>
                        <td><strong>MIG (GPU)</strong></td>
                        <td class="medium">~5-10%</td>
                        <td class="good">High (hardware)</td>
                        <td class="medium">Fixed partitions</td>
                        <td>Inference serving</td>
                    </tr>
                    <tr>
                        <td><strong>NVMe Namespaces</strong></td>
                        <td class="good">~0%</td>
                        <td class="medium">Logical only</td>
                        <td class="good">High</td>
                        <td>Storage isolation</td>
                    </tr>
                    <tr>
                        <td><strong>vGPU (GRID)</strong></td>
                        <td class="medium">~10-20%</td>
                        <td class="medium">Software</td>
                        <td class="good">High</td>
                        <td>VDI, shared inference</td>
                    </tr>
                </tbody>
            </table>

            <h3 class="subsection-title">QoS and Noisy Neighbor Mitigation</h3>
            
            <div class="info-box warning">
                <strong>Ã¢Å¡Â Ã¯Â¸Â Multi-tenant Challenge:</strong> A single tenant's checkpoint storm can saturate shared NVMe bandwidth, causing latency spikes for all tenants. Without proper QoS, "noisy neighbor" effects can increase P99 latency 10-50Ã—.
            </div>

            <pre><code><span style="color: #6a9955;"># NVMe WRR (Weighted Round Robin) arbitration for tenant QoS</span>
<span style="color: #6a9955;"># Configure submission queue priorities</span>

<span style="color: #6a9955;"># Get current arbitration settings</span>
nvme get-feature /dev/nvme0 -f 1 -H  <span style="color: #6a9955;"># Feature ID 1 = Arbitration</span>

<span style="color: #6a9955;"># NVMe supports 3 priority levels with WRR:</span>
<span style="color: #6a9955;"># - Urgent (highest priority, use sparingly)</span>
<span style="color: #6a9955;"># - High, Medium, Low (weighted round-robin)</span>

<span style="color: #6a9955;"># Create per-tenant queues with different priorities:</span>
<span style="color: #6a9955;"># - Tenant A (batch training): Low priority queues</span>
<span style="color: #6a9955;"># - Tenant B (inference): High priority queues</span>
<span style="color: #6a9955;"># - Tenant C (interactive): Urgent priority (limited)</span>

<span style="color: #6a9955;"># Set WRR weights (via driver or nvme-cli)</span>
<span style="color: #6a9955;"># HPW:MPW:LPW ratio determines bandwidth allocation</span>
nvme set-feature /dev/nvme0 -f 1 -v 0x07050301  <span style="color: #6a9955;"># HPW=7, MPW=5, LPW=3, burst=1</span>

<span style="color: #6a9955;"># Linux cgroup v2 I/O controller for container-level limits</span>
<span style="color: #6a9955;"># /sys/fs/cgroup/tenant_a/io.max</span>
<span style="color: #569cd6;">echo</span> <span style="color: #ce9178;">"259:0 rbps=2000000000 wbps=1000000000 riops=100000 wiops=50000"</span> > /sys/fs/cgroup/tenant_a/io.max

<span style="color: #6a9955;"># Kubernetes resource limits (for pods)</span>
<span style="color: #6a9955;"># Note: Requires device plugin support for NVMe QoS</span>
</code></pre>

            <table>
                <thead>
                    <tr>
                        <th>QoS Mechanism</th>
                        <th>Granularity</th>
                        <th>Enforcement</th>
                        <th>GPU Workload Fit</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>NVMe WRR Arbitration</strong></td>
                        <td>Per-queue</td>
                        <td class="good">Hardware (SSD)</td>
                        <td class="good">Best: assign queues per tenant</td>
                    </tr>
                    <tr>
                        <td><strong>Linux cgroup io.max</strong></td>
                        <td>Per-cgroup</td>
                        <td class="medium">Kernel throttling</td>
                        <td class="medium">Container isolation</td>
                    </tr>
                    <tr>
                        <td><strong>SR-IOV VF rate limiting</strong></td>
                        <td>Per-VF</td>
                        <td class="good">Hardware</td>
                        <td class="good">VM isolation (if supported)</td>
                    </tr>
                    <tr>
                        <td><strong>NVMe Namespace bandwidth</strong></td>
                        <td>Per-namespace</td>
                        <td class="medium">Firmware</td>
                        <td class="medium">Logical separation only</td>
                    </tr>
                </tbody>
            </table>

            <div class="info-box production">
                <strong>Ã°Å¸ÂÂ­ Production Pattern:</strong> For GPU clusters with mixed training/inference:
                <ol style="margin: 10px 0; padding-left: 20px;">
                    <li>Dedicate SSDs to high-priority inference (no sharing)</li>
                    <li>Use NVMe WRR with per-tenant queues for training pool</li>
                    <li>Stagger checkpoint times across training jobs (reduces burst contention)</li>
                    <li>Monitor per-tenant latency via eBPF and alert on P99 > 2Ã— baseline</li>
                    <li>Reserve 20% SSD capacity headroom to prevent GC-induced latency spikes</li>
                </ol>
            </div>

        </section>

        <!-- Section 5: Advanced NVMe Features -->
        <section class="section" id="nvme-advanced">
            <h2 class="section-title">Ã¢Å¡Â¡ Advanced NVMe Features</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â NVMe Evolution:</strong> I chaired storage standards meetings when NVMe was being designed. 
                The original spec was simple: queues, commands, completions. Now it's grown to include 
                management interfaces, persistent memory regions, and controller memory buffers. 
                Here's what matters for GPU integration.
            </div>

            <h3 class="subsection-title">NVMe-MI (Management Interface)</h3>
            
            <p>NVMe-MI provides out-of-band management for NVMe devices â€” critical for large-scale deployments 
            where you need to monitor/manage storage without host OS involvement.</p>

            <pre><code><span style="color: #6a9955;"># NVMe-MI commands for GPU-storage infrastructure management</span>

<span style="color: #6a9955;"># NVMe-MI over SMBus (out-of-band)</span>
<span style="color: #6a9955;"># Allows BMC to query SSD health without host OS</span>

<span style="color: #6a9955;"># Using nvme-cli with MI support</span>
<span style="color: #6a9955;"># List MI capabilities</span>
nvme mi-cmd /dev/nvme0 --opcode=0x00  <span style="color: #6a9955;"># Read NVMe-MI DS</span>

<span style="color: #6a9955;"># Get Controller Health Status (out-of-band)</span>
nvme mi-cmd /dev/nvme0 --opcode=0x02  <span style="color: #6a9955;"># Health Status Poll</span>

<span style="color: #6a9955;"># NVMe-MI over MCTP (Management Component Transport Protocol)</span>
<span style="color: #6a9955;"># Enables network-based SSD management</span>

<span style="color: #6a9955;"># Key NVMe-MI commands for production:</span>
<span style="color: #6a9955;"># 1. Health Status Poll - Proactive health monitoring</span>
<span style="color: #6a9955;"># 2. Configuration Get/Set - Remote configuration</span>
<span style="color: #6a9955;"># 3. Reset - Remote device reset without host</span>
<span style="color: #6a9955;"># 4. VPD Read - Asset management</span>

<span style="color: #6a9955;"># Example: BMC monitoring script</span>
<span style="color: #c586c0;">import</span> mctp
<span style="color: #c586c0;">import</span> nvme_mi

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">check_ssd_health_oob</span>(ssd_eid):
    <span style="color: #ce9178;">"""Check SSD health via out-of-band NVMe-MI"""</span>
    transport = mctp.SMBusTransport()
    mi = nvme_mi.NVMeMI(transport, ssd_eid)
    
    <span style="color: #6a9955;"># Get health status without involving host CPU</span>
    health = mi.health_status_poll()
    
    <span style="color: #c586c0;">return</span> {
        <span style="color: #ce9178;">'temperature'</span>: health.composite_temp,
        <span style="color: #ce9178;">'spare'</span>: health.available_spare,
        <span style="color: #ce9178;">'wear'</span>: health.percentage_used,
        <span style="color: #ce9178;">'critical_warnings'</span>: health.critical_warning
    }</code></pre>

            <h3 class="subsection-title">PMR (Persistent Memory Region)</h3>
            
            <p>PMR exposes a region of the SSD's internal memory (usually DRAM-backed with battery) 
            as a directly-accessible memory-mapped region. This is a potential game-changer for GPU integration.</p>

            <pre><code><span style="color: #6a9955;"># PMR (Persistent Memory Region) configuration</span>

<span style="color: #6a9955;"># Check if NVMe device supports PMR</span>
nvme id-ctrl /dev/nvme0 | grep -i pmr
<span style="color: #6a9955;"># PMRCTL: 0x1  (PMR supported)</span>

<span style="color: #6a9955;"># Get PMR capabilities</span>
nvme id-ctrl /dev/nvme0 -H | grep -A 10 "Persistent Memory"

<span style="color: #6a9955;"># PMR provides:</span>
<span style="color: #6a9955;"># - Memory-mapped access (no command queue overhead)</span>
<span style="color: #6a9955;"># - Persistent across power cycles (if battery-backed)</span>
<span style="color: #6a9955;"># - Low latency (~500ns vs ~5Âµs for regular NVMe)</span>

<span style="color: #6a9955;"># Enable PMR</span>
nvme set-feature /dev/nvme0 -f 0x0E -v 1  <span style="color: #6a9955;"># Enable PMR</span>

<span style="color: #6a9955;"># Map PMR to user space</span>
<span style="color: #c586c0;">import</span> mmap
<span style="color: #c586c0;">import</span> os

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">map_pmr</span>(nvme_device):
    <span style="color: #ce9178;">"""Map NVMe PMR to user space for direct access"""</span>
    
    <span style="color: #6a9955;"># Get PMR BAR from sysfs</span>
    pmr_resource = f<span style="color: #ce9178;">"/sys/class/nvme/{nvme_device}/device/resource4"</span>
    
    <span style="color: #c586c0;">with</span> open(pmr_resource, <span style="color: #ce9178;">'r+b'</span>) <span style="color: #c586c0;">as</span> f:
        pmr_size = os.path.getsize(pmr_resource)
        pmr_map = mmap.mmap(f.fileno(), pmr_size, 
                           mmap.MAP_SHARED, 
                           mmap.PROT_READ | mmap.PROT_WRITE)
    
    <span style="color: #c586c0;">return</span> pmr_map

<span style="color: #6a9955;"># GPU access to PMR (theoretical - requires driver support)</span>
<span style="color: #6a9955;"># PMR could be mapped to GPU BAR for ultra-low-latency access</span>
<span style="color: #6a9955;"># This would bypass NVMe command queues entirely</span></code></pre>

            <div class="info-box insight">
                <strong>ğŸ’¡ PMR Potential for GPUs:</strong> If PMR could be exposed to GPUs via BAR mapping, 
                you'd get ~500ns access to persistent storage â€” 10Ã— faster than regular NVMe commands. 
                This is what CXL.mem promises but PMR exists today. The challenge is getting GPU drivers 
                to recognize and use PMR regions.
            </div>

            <h3 class="subsection-title">CMB (Controller Memory Buffer) Deep Dive</h3>
            
            <p>CMB allows host software to place submission queues and PRPs in the controller's internal memory, 
            reducing PCIe round-trips.</p>

            <pre><code><span style="color: #6a9955;"># CMB (Controller Memory Buffer) deep configuration</span>

<span style="color: #6a9955;"># Check CMB support</span>
nvme id-ctrl /dev/nvme0 | grep -i cmb
<span style="color: #6a9955;"># CMBSZ: 0x100000 (CMB size in 4KB units)</span>
<span style="color: #6a9955;"># CMBLOC: Shows supported uses</span>

<span style="color: #6a9955;"># CMB can be used for:</span>
<span style="color: #6a9955;"># - Submission Queue (SQS) - Reduces SQ fetch latency</span>
<span style="color: #6a9955;"># - Completion Queue (CQS) - Not commonly used</span>
<span style="color: #6a9955;"># - PRP List (LISTS) - Reduces PRP fetch overhead</span>
<span style="color: #6a9955;"># - Read Data (RDS) - Avoid PCIe write for small reads</span>
<span style="color: #6a9955;"># - Write Data (WDS) - Avoid PCIe read for small writes</span>

<span style="color: #6a9955;"># Decode CMB capabilities</span>
nvme id-ctrl /dev/nvme0 -H | grep -A 15 "Controller Memory Buffer"

<span style="color: #6a9955;"># CMB for GPU optimization:</span>
<span style="color: #6a9955;"># 1. Place SQ in CMB - GPU writes commands directly to SSD memory</span>
<span style="color: #6a9955;"># 2. Place small write data in CMB - Reduces PCIe traffic for metadata</span>
<span style="color: #6a9955;"># 3. Place PRP lists in CMB - Critical for large transfers with many PRPs</span>

<span style="color: #c586c0;">class</span> <span style="color: #4ec9b0;">CMBOptimizedQueue</span>:
    <span style="color: #ce9178;">"""NVMe queue using CMB for optimal GPU access"""</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__init__</span>(self, nvme_fd, queue_id, cmb_base):
        self.nvme_fd = nvme_fd
        self.queue_id = queue_id
        
        <span style="color: #6a9955;"># Map CMB region</span>
        self.cmb = mmap.mmap(nvme_fd, CMB_SIZE, offset=cmb_base)
        
        <span style="color: #6a9955;"># Allocate SQ in CMB</span>
        self.sq_offset = <span style="color: #b5cea8;">0</span>
        self.sq_size = <span style="color: #b5cea8;">64</span> * <span style="color: #b5cea8;">1024</span>  <span style="color: #6a9955;"># 1024 entries Ã— 64B</span>
        
        <span style="color: #6a9955;"># Allocate PRP pool in CMB</span>
        self.prp_offset = self.sq_size
        self.prp_pool_size = <span style="color: #b5cea8;">256</span> * <span style="color: #b5cea8;">4096</span>  <span style="color: #6a9955;"># 256 PRP pages</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">submit_command</span>(self, cmd):
        <span style="color: #ce9178;">"""Submit command via CMB-resident SQ"""</span>
        <span style="color: #6a9955;"># Write directly to CMB - no host memory involved</span>
        sq_entry_offset = self.sq_offset + (self.sq_tail * <span style="color: #b5cea8;">64</span>)
        self.cmb[sq_entry_offset:sq_entry_offset+<span style="color: #b5cea8;">64</span>] = cmd.to_bytes()
        
        <span style="color: #6a9955;"># Ring doorbell</span>
        self.sq_tail = (self.sq_tail + <span style="color: #b5cea8;">1</span>) % self.sq_depth
        self._ring_doorbell()</code></pre>

            <h4 style="color: #00ff88; margin: 25px 0 15px 0;">CMB + GPU: Direct P2P Command Submission</h4>
            
            <p>The real power of CMB for GPU workloads is enabling P2P writes from GPU to SSD command queues, 
            bypassing host memory entirely:</p>

            <pre><code><span style="color: #6a9955;">// ============================================</span>
<span style="color: #6a9955;">// GPU-Direct CMB Access: P2P Queue Submission</span>
<span style="color: #6a9955;">// ============================================</span>
<span style="color: #6a9955;">// This enables GPU threads to submit NVMe commands directly</span>
<span style="color: #6a9955;">// to SSD controller memory without any CPU involvement!</span>

<span style="color: #569cd6;">#include</span> &lt;cuda_runtime.h&gt;

<span style="color: #c586c0;">struct</span> nvme_sq_entry {
    <span style="color: #4ec9b0;">uint32_t</span> cdw0;     <span style="color: #6a9955;">// Command Dword 0 (opcode, etc.)</span>
    <span style="color: #4ec9b0;">uint32_t</span> nsid;     <span style="color: #6a9955;">// Namespace ID</span>
    <span style="color: #4ec9b0;">uint64_t</span> rsvd;
    <span style="color: #4ec9b0;">uint64_t</span> mptr;     <span style="color: #6a9955;">// Metadata pointer</span>
    <span style="color: #4ec9b0;">uint64_t</span> prp1;     <span style="color: #6a9955;">// PRP Entry 1 (data pointer)</span>
    <span style="color: #4ec9b0;">uint64_t</span> prp2;     <span style="color: #6a9955;">// PRP Entry 2</span>
    <span style="color: #4ec9b0;">uint32_t</span> cdw10_15[<span style="color: #b5cea8;">6</span>]; <span style="color: #6a9955;">// Command-specific</span>
};

<span style="color: #6a9955;">// Host setup: Map CMB to GPU address space</span>
<span style="color: #4ec9b0;">void</span>* setup_cmb_for_gpu(<span style="color: #4ec9b0;">int</span> nvme_fd, <span style="color: #4ec9b0;">int</span> gpu_id) {
    <span style="color: #6a9955;">// 1. Get CMB BAR address from NVMe controller</span>
    <span style="color: #c586c0;">struct</span> nvme_bar *bar = get_nvme_bar(nvme_fd);
    <span style="color: #4ec9b0;">uint64_t</span> cmb_bar_addr = bar->cmbloc &amp; ~<span style="color: #b5cea8;">0xFFF</span>;  <span style="color: #6a9955;">// 4KB aligned</span>
    <span style="color: #4ec9b0;">size_t</span> cmb_size = (bar->cmbsz &amp; <span style="color: #b5cea8;">0xFFFFF</span>) * <span style="color: #b5cea8;">4096</span>;  <span style="color: #6a9955;">// In 4KB units</span>
    
    <span style="color: #6a9955;">// 2. Register CMB region with CUDA for P2P access</span>
    <span style="color: #4ec9b0;">void</span>* cmb_gpu_ptr;
    cudaSetDevice(gpu_id);
    
    <span style="color: #6a9955;">// Enable P2P access from GPU to NVMe PCIe BAR</span>
    CUresult res = cuMemHostRegister(
        (<span style="color: #4ec9b0;">void</span>*)cmb_bar_addr, 
        cmb_size,
        CU_MEMHOSTREGISTER_IOMEMORY  <span style="color: #6a9955;">// Critical: marks as I/O memory</span>
    );
    
    <span style="color: #6a9955;">// 3. Get device pointer for kernel use</span>
    cudaHostGetDevicePointer(&cmb_gpu_ptr, (<span style="color: #4ec9b0;">void</span>*)cmb_bar_addr, <span style="color: #b5cea8;">0</span>);
    
    <span style="color: #c586c0;">return</span> cmb_gpu_ptr;
}

<span style="color: #6a9955;">// GPU kernel: Submit NVMe read command directly to CMB-resident SQ</span>
__global__ <span style="color: #4ec9b0;">void</span> gpu_submit_nvme_read(
    <span style="color: #c586c0;">volatile</span> <span style="color: #c586c0;">struct</span> nvme_sq_entry* cmb_sq,  <span style="color: #6a9955;">// CMB-resident SQ</span>
    <span style="color: #c586c0;">volatile</span> <span style="color: #4ec9b0;">uint32_t</span>* doorbell,            <span style="color: #6a9955;">// SQ tail doorbell</span>
    <span style="color: #4ec9b0;">uint64_t</span>* gpu_data_buffers,             <span style="color: #6a9955;">// GPU memory for data</span>
    <span style="color: #4ec9b0;">uint64_t</span>* lba_list,                     <span style="color: #6a9955;">// LBAs to read</span>
    <span style="color: #4ec9b0;">int</span> num_commands,
    <span style="color: #4ec9b0;">int</span> sq_depth
) {
    <span style="color: #4ec9b0;">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;
    <span style="color: #c586c0;">if</span> (tid >= num_commands) <span style="color: #c586c0;">return</span>;
    
    <span style="color: #6a9955;">// Each thread builds and submits one NVMe command</span>
    <span style="color: #4ec9b0;">int</span> sq_slot = tid % sq_depth;  <span style="color: #6a9955;">// Simple slot allocation (production needs atomic)</span>
    
    <span style="color: #6a9955;">// Build NVMe Read command (opcode 0x02)</span>
    <span style="color: #c586c0;">struct</span> nvme_sq_entry cmd;
    cmd.cdw0 = <span style="color: #b5cea8;">0x02</span> | (tid << <span style="color: #b5cea8;">16</span>);  <span style="color: #6a9955;">// Read opcode + command ID</span>
    cmd.nsid = <span style="color: #b5cea8;">1</span>;
    cmd.prp1 = gpu_data_buffers[tid];  <span style="color: #6a9955;">// GPU memory address!</span>
    cmd.prp2 = <span style="color: #b5cea8;">0</span>;  <span style="color: #6a9955;">// For small reads, PRP1 is enough</span>
    cmd.cdw10_15[<span style="color: #b5cea8;">0</span>] = lba_list[tid] &amp; <span style="color: #b5cea8;">0xFFFFFFFF</span>;         <span style="color: #6a9955;">// SLBA low</span>
    cmd.cdw10_15[<span style="color: #b5cea8;">1</span>] = (lba_list[tid] >> <span style="color: #b5cea8;">32</span>) &amp; <span style="color: #b5cea8;">0xFFFFFFFF</span>; <span style="color: #6a9955;">// SLBA high</span>
    cmd.cdw10_15[<span style="color: #b5cea8;">2</span>] = <span style="color: #b5cea8;">0</span>;  <span style="color: #6a9955;">// NLB = 0 means 1 block (4KB)</span>
    
    <span style="color: #6a9955;">// Write command directly to CMB (P2P PCIe write!)</span>
    cmb_sq[sq_slot] = cmd;
    
    <span style="color: #6a9955;">// Memory fence to ensure command is visible</span>
    __threadfence_system();
    
    <span style="color: #6a9955;">// Ring doorbell (only one thread per batch should do this)</span>
    <span style="color: #c586c0;">if</span> (threadIdx.x == <span style="color: #b5cea8;">0</span>) {
        atomicAdd((<span style="color: #4ec9b0;">unsigned int</span>*)doorbell, blockDim.x);
    }
}

<span style="color: #6a9955;">// Host launch code</span>
<span style="color: #4ec9b0;">void</span> launch_gpu_nvme_reads(<span style="color: #4ec9b0;">int</span> num_reads) {
    <span style="color: #6a9955;">// CMB already mapped to GPU address space</span>
    <span style="color: #4ec9b0;">void</span>* cmb_sq_gpu = setup_cmb_for_gpu(nvme_fd, <span style="color: #b5cea8;">0</span>);
    <span style="color: #4ec9b0;">void</span>* doorbell_gpu = cmb_sq_gpu + DOORBELL_OFFSET;
    
    <span style="color: #6a9955;">// Launch kernel - GPU threads write directly to SSD!</span>
    <span style="color: #4ec9b0;">int</span> threads_per_block = <span style="color: #b5cea8;">256</span>;
    <span style="color: #4ec9b0;">int</span> blocks = (num_reads + threads_per_block - <span style="color: #b5cea8;">1</span>) / threads_per_block;
    
    gpu_submit_nvme_read<<<blocks, threads_per_block>>>(
        (<span style="color: #c586c0;">struct</span> nvme_sq_entry*)cmb_sq_gpu,
        (<span style="color: #4ec9b0;">uint32_t</span>*)doorbell_gpu,
        gpu_data_buffers,
        lba_list,
        num_reads,
        SQ_DEPTH
    );
    
    <span style="color: #6a9955;">// Data path: GPU Ã¢â€ â€™ CMB (P2P write)</span>
    <span style="color: #6a9955;">// SSD reads command from CMB (internal)</span>
    <span style="color: #6a9955;">// SSD DMAs data to GPU memory (P2P read)</span>
    <span style="color: #6a9955;">// Zero CPU involvement in data path!</span>
}</code></pre>

            <div class="info-box warning">
                <strong>Ã¢Å¡Â Ã¯Â¸Â CMB + GPU Reality Check:</strong>
                <ul style="margin-top: 10px; margin-left: 20px;">
                    <li><strong>CMB sizes are small:</strong> Typically 64KB-4MB. Can't hold many queues + PRP lists.</li>
                    <li><strong>P2P BAR mapping:</strong> Requires IOMMU configuration and matching PCIe topology.</li>
                    <li><strong>Doorbell bottleneck:</strong> All threads hitting same doorbell causes serialization.</li>
                    <li><strong>Not in GDS:</strong> cuFile uses different path; CMB P2P is custom implementation.</li>
                    <li><strong>Debugging is hard:</strong> PCIe analyzer often needed for P2P issues.</li>
                </ul>
            </div>

            <pre><code><span style="color: #6a9955;"># Verify CMB P2P path is working</span>
$ nvidia-smi topo -m
        GPU0    GPU1    mlx5_0  nvme0
GPU0    X       NV12    PIX     PHB     <span style="color: #6a9955;"># PHB = through PCIe Host Bridge (OK)</span>
GPU1    NV12    X       SYS     SYS     <span style="color: #6a9955;"># SYS = through CPU (slower)</span>
nvme0   PHB     SYS     PIX     X

<span style="color: #6a9955;"># Check CMB is exposed as BAR</span>
$ lspci -vvv -s 01:00.0 | grep -A5 "Region"
Region 0: Memory at f0000000 (64-bit, non-prefetchable) [size=16K]  <span style="color: #6a9955;"># NVMe registers</span>
Region 4: Memory at f0010000 (64-bit, prefetchable) [size=4M]       <span style="color: #6a9955;"># CMB!</span>

<span style="color: #6a9955;"># Performance comparison</span>
<span style="color: #6a9955;"># Traditional (via host memory):  GPU Ã¢â€ â€™ Host RAM Ã¢â€ â€™ SSD</span>
<span style="color: #6a9955;">#   Latency: ~15-25Âµs for command submission</span>
<span style="color: #6a9955;"># CMB P2P:                        GPU Ã¢â€ â€™ CMB (SSD memory)</span>
<span style="color: #6a9955;">#   Latency: ~3-8Âµs for command submission</span>
<span style="color: #6a9955;"># Savings: 50-70% command submission latency reduction</span></code></pre>

            <table>
                <thead>
                    <tr>
                        <th>NVMe Feature</th>
                        <th>GPU Benefit</th>
                        <th>Availability</th>
                        <th>Implementation Complexity</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CMB for SQ</strong></td>
                        <td>Reduce command submission latency</td>
                        <td class="medium">Many enterprise SSDs</td>
                        <td class="medium">Moderate (kernel support exists)</td>
                    </tr>
                    <tr>
                        <td><strong>CMB for PRP</strong></td>
                        <td>Eliminate PRP fetch from host</td>
                        <td class="medium">Select SSDs</td>
                        <td class="medium">Moderate</td>
                    </tr>
                    <tr>
                        <td><strong>PMR</strong></td>
                        <td>~500ns persistent storage access</td>
                        <td class="poor">Rare (specialized SSDs)</td>
                        <td class="poor">High (custom drivers)</td>
                    </tr>
                    <tr>
                        <td><strong>NVMe-MI</strong></td>
                        <td>Out-of-band health monitoring</td>
                        <td class="good">Enterprise SSDs</td>
                        <td class="good">Low (BMC integration)</td>
                    </tr>
                    <tr>
                        <td><strong>SGL</strong></td>
                        <td>Better scatter-gather for GPU memory</td>
                        <td class="good">Most modern SSDs</td>
                        <td class="good">Low (GDS uses this)</td>
                    </tr>
                </tbody>
            </table>

        </section>

        <!-- Section 5.5: Write Amplification & Checkpoint Storms -->
        <section class="section" id="write-amplification">
            <h2 class="section-title">Ã°Å¸â€œÂ Write Amplification & Checkpoint Storms</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â The Hidden Performance Killer:</strong> I've seen AI training clusters lose 50% write 
                performance over 6 months with no obvious cause. The culprit? SSD garbage collection fighting 
                with checkpoint storms. Understanding write amplification is critical for sustained performance.
            </div>

            <h3 class="subsection-title">What is Write Amplification?</h3>
            
            <p>Write Amplification Factor (WAF) measures how much more data the SSD writes internally versus 
            what the host requested. A WAF of 3Ã— means writing 100GB causes 300GB of NAND writes.</p>
            
            <pre><code><span style="color: #6a9955;"># Calculate Write Amplification from SMART data</span>
<span style="color: #c586c0;">import</span> subprocess
<span style="color: #c586c0;">import</span> json

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">calculate_waf</span>(device):
    <span style="color: #ce9178;">"""Calculate Write Amplification Factor from SMART logs"""</span>
    result = subprocess.run(
        [<span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'smart-log'</span>, device, <span style="color: #ce9178;">'-o'</span>, <span style="color: #ce9178;">'json'</span>],
        capture_output=<span style="color: #569cd6;">True</span>, text=<span style="color: #569cd6;">True</span>
    )
    smart = json.loads(result.stdout)
    
    <span style="color: #6a9955;"># Host bytes written (what you asked for)</span>
    host_bytes = smart[<span style="color: #ce9178;">'data_units_written'</span>] * <span style="color: #b5cea8;">512</span> * <span style="color: #b5cea8;">1000</span>  <span style="color: #6a9955;"># In bytes</span>
    
    <span style="color: #6a9955;"># NAND bytes written (vendor-specific, check ID log)</span>
    <span style="color: #6a9955;"># This example assumes Intel/Samsung format</span>
    vs_result = subprocess.run(
        [<span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'intel'</span>, <span style="color: #ce9178;">'smart-log-add'</span>, device, <span style="color: #ce9178;">'-o'</span>, <span style="color: #ce9178;">'json'</span>],
        capture_output=<span style="color: #569cd6;">True</span>, text=<span style="color: #569cd6;">True</span>
    )
    vs_smart = json.loads(vs_result.stdout)
    nand_bytes = vs_smart.get(<span style="color: #ce9178;">'nand_bytes_written'</span>, host_bytes)
    
    waf = nand_bytes / host_bytes <span style="color: #c586c0;">if</span> host_bytes > <span style="color: #b5cea8;">0</span> <span style="color: #c586c0;">else</span> <span style="color: #b5cea8;">1.0</span>
    
    <span style="color: #c586c0;">return</span> {
        <span style="color: #ce9178;">'host_written_tb'</span>: host_bytes / (<span style="color: #b5cea8;">1024</span>**<span style="color: #b5cea8;">4</span>),
        <span style="color: #ce9178;">'nand_written_tb'</span>: nand_bytes / (<span style="color: #b5cea8;">1024</span>**<span style="color: #b5cea8;">4</span>),
        <span style="color: #ce9178;">'waf'</span>: waf,
        <span style="color: #ce9178;">'status'</span>: <span style="color: #ce9178;">'GOOD'</span> <span style="color: #c586c0;">if</span> waf < <span style="color: #b5cea8;">2.0</span> <span style="color: #c586c0;">else</span> (<span style="color: #ce9178;">'WARNING'</span> <span style="color: #c586c0;">if</span> waf < <span style="color: #b5cea8;">3.0</span> <span style="color: #c586c0;">else</span> <span style="color: #ce9178;">'BAD'</span>)
    }

<span style="color: #6a9955;"># Check all SSDs</span>
<span style="color: #c586c0;">for</span> i <span style="color: #c586c0;">in</span> range(<span style="color: #b5cea8;">8</span>):
    stats = calculate_waf(f<span style="color: #ce9178;">'/dev/nvme{i}'</span>)
    print(f<span style="color: #ce9178;">"nvme{i}: WAF={stats['waf']:.2f} ({stats['status']})"</span>)</code></pre>

            <h3 class="subsection-title">Checkpoint Storm Impact on SSDs</h3>
            
            <p>AI training creates periodic write bursts during checkpointing that interact badly with 
            SSD garbage collection:</p>
            
            <pre><code><span style="color: #6a9955;"># Checkpoint storm timeline (what happens inside the SSD)</span>
<span style="color: #ce9178;">"""
Time     Event                          SSD Internal State
Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬
0s       Training iteration 1000        GC: Idle, WAF: 1.0
         Checkpoint triggered!          
0.1s     100GB write burst starts       GC: Active (blocks filling)
1.5s     Checkpoint complete            GC: Heavy (moving valid pages)
         
2s       Training resumes               GC: Still running in background
5s       GC completes                   WAF for this burst: 2.5Ã—
         
...      Training continues             GC: Idle
         
600s     Checkpoint triggered!          GC: Conflict begins
         Another 100GB burst            
600.1s   Write burst + GC collision     LATENCY SPIKE! 50msÃ¢â€ â€™500ms
601s     Training stalls waiting        WAF: 4Ã— (GC competing with writes)
"""</span>

<span style="color: #6a9955;"># Detecting checkpoint-induced latency spikes</span>
<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">monitor_checkpoint_impact</span>(device, checkpoint_interval_sec=<span style="color: #b5cea8;">600</span>):
    <span style="color: #ce9178;">"""Track write latency around checkpoint windows"""</span>
    <span style="color: #c586c0;">import</span> time
    
    samples = []
    checkpoint_num = <span style="color: #b5cea8;">0</span>
    
    <span style="color: #c586c0;">while</span> <span style="color: #569cd6;">True</span>:
        <span style="color: #6a9955;"># Sample write latency (from nvme latency histogram)</span>
        lat = get_write_latency_p99(device)
        samples.append({
            <span style="color: #ce9178;">'time'</span>: time.time(),
            <span style="color: #ce9178;">'latency_us'</span>: lat,
            <span style="color: #ce9178;">'near_checkpoint'</span>: (time.time() % checkpoint_interval_sec) < <span style="color: #b5cea8;">30</span>
        })
        
        <span style="color: #6a9955;"># Alert on spike during checkpoint window</span>
        <span style="color: #c586c0;">if</span> lat > <span style="color: #b5cea8;">100000</span> <span style="color: #c586c0;">and</span> samples[-<span style="color: #b5cea8;">1</span>][<span style="color: #ce9178;">'near_checkpoint'</span>]:
            print(f<span style="color: #ce9178;">"Ã¢Å¡Â Ã¯Â¸Â Checkpoint storm detected! P99 lat: {lat/1000:.1f}ms"</span>)
        
        time.sleep(<span style="color: #b5cea8;">1</span>)</code></pre>

            <h3 class="subsection-title">Mitigation Strategies</h3>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Strategy</th>
                        <th>Implementation</th>
                        <th>WAF Reduction</th>
                        <th>Complexity</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Over-provisioning</strong></td>
                        <td>Leave 20-30% capacity unused</td>
                        <td>30-50%</td>
                        <td class="good">Low</td>
                    </tr>
                    <tr>
                        <td><strong>ZNS (Zoned Namespaces)</strong></td>
                        <td>Sequential-only zones, minimal GC</td>
                        <td>Up to 4Ã—</td>
                        <td class="warning">Medium</td>
                    </tr>
                    <tr>
                        <td><strong>Staggered checkpoints</strong></td>
                        <td>Time-shift across GPUs</td>
                        <td>20-30%</td>
                        <td class="good">Low</td>
                    </tr>
                    <tr>
                        <td><strong>Dedicated checkpoint SSD</strong></td>
                        <td>Separate write-heavy workload</td>
                        <td>Variable</td>
                        <td class="good">Low</td>
                    </tr>
                    <tr>
                        <td><strong>FDP (Flexible Data Placement)</strong></td>
                        <td>Application-guided placement</td>
                        <td>Up to 3Ã—</td>
                        <td class="warning">Medium</td>
                    </tr>
                </tbody>
            </table>

            <pre><code><span style="color: #6a9955;"># Staggered checkpoint implementation</span>
<span style="color: #c586c0;">import</span> torch.distributed <span style="color: #c586c0;">as</span> dist

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">save_checkpoint_staggered</span>(model, optimizer, epoch, stagger_sec=<span style="color: #b5cea8;">10</span>):
    <span style="color: #ce9178;">"""Save checkpoints with time-staggering to reduce SSD stress"""</span>
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    
    <span style="color: #6a9955;"># Stagger start times based on rank</span>
    delay = (rank % <span style="color: #b5cea8;">8</span>) * stagger_sec  <span style="color: #6a9955;"># 8 SSDs, stagger across them</span>
    time.sleep(delay)
    
    <span style="color: #6a9955;"># Now save</span>
    checkpoint = {
        <span style="color: #ce9178;">'epoch'</span>: epoch,
        <span style="color: #ce9178;">'model_state'</span>: model.state_dict(),
        <span style="color: #ce9178;">'optimizer_state'</span>: optimizer.state_dict(),
    }
    
    torch.save(checkpoint, f<span style="color: #ce9178;">'/mnt/nvme{rank % 8}/ckpt_rank{rank}_epoch{epoch}.pt'</span>)
    
    <span style="color: #6a9955;"># Wait for all ranks to complete</span>
    dist.barrier()

<span style="color: #6a9955;"># Over-provisioning check script</span>
<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">check_overprovision</span>(device):
    <span style="color: #ce9178;">"""Verify sufficient over-provisioning for checkpoint workloads"""</span>
    <span style="color: #c586c0;">import</span> os
    
    <span style="color: #6a9955;"># Get total and used capacity</span>
    statvfs = os.statvfs(device)
    total_gb = (statvfs.f_blocks * statvfs.f_frsize) / (<span style="color: #b5cea8;">1024</span>**<span style="color: #b5cea8;">3</span>)
    used_gb = ((statvfs.f_blocks - statvfs.f_bfree) * statvfs.f_frsize) / (<span style="color: #b5cea8;">1024</span>**<span style="color: #b5cea8;">3</span>)
    
    op_percent = <span style="color: #b5cea8;">100</span> * (<span style="color: #b5cea8;">1</span> - used_gb / total_gb)
    
    status = <span style="color: #ce9178;">'GOOD'</span> <span style="color: #c586c0;">if</span> op_percent >= <span style="color: #b5cea8;">25</span> <span style="color: #c586c0;">else</span> (<span style="color: #ce9178;">'WARNING'</span> <span style="color: #c586c0;">if</span> op_percent >= <span style="color: #b5cea8;">15</span> <span style="color: #c586c0;">else</span> <span style="color: #ce9178;">'CRITICAL'</span>)
    
    print(f<span style="color: #ce9178;">"Over-provisioning: {op_percent:.1f}% ({status})"</span>)
    print(f<span style="color: #ce9178;">"Recommendation: Keep Ã¢â€°Â¥25% free for checkpoint-heavy workloads"</span>)
    
    <span style="color: #c586c0;">return</span> op_percent</code></pre>

            <!-- ZNS/FDP Working Examples -->
            <h3 class="subsection-title">ZNS (Zoned Namespaces) Working Examples</h3>
            
            <p>ZNS SSDs eliminate garbage collection by requiring sequential writes within zones. This is ideal for checkpoint workloads which are naturally sequential:</p>
            
            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># ZNS SSD Discovery and Zone Management</span>
<span style="color: #6a9955;"># ============================================</span>

<span style="color: #6a9955;"># Check if SSD supports ZNS</span>
$ nvme id-ns /dev/nvme0n1 | grep -i csi
csi     : 2      <span style="color: #6a9955;"># CSI=2 means Zoned Namespace Command Set</span>

<span style="color: #6a9955;"># Get zone information</span>
$ nvme zns report-zones /dev/nvme0n1 --descs=10
nr_zones: 4096
SLBA: 0x0        WP: 0x0        Cap: 0x40000    State: EMPTY    Type: SWR  
SLBA: 0x40000    WP: 0x40000    Cap: 0x40000    State: EMPTY    Type: SWR
SLBA: 0x80000    WP: 0x60000    Cap: 0x40000    State: IMP_OPEN Type: SWR
SLBA: 0xc0000    WP: 0xc0000    Cap: 0x40000    State: FULL     Type: SWR

<span style="color: #6a9955;"># Key fields:</span>
<span style="color: #6a9955;"># - SLBA: Starting LBA of zone</span>
<span style="color: #6a9955;"># - WP: Write Pointer (next write location)</span>
<span style="color: #6a9955;"># - Cap: Zone capacity in 512B sectors (0x40000 = 256MB)</span>
<span style="color: #6a9955;"># - State: EMPTY, IMP_OPEN, EXP_OPEN, CLOSED, FULL, READ_ONLY</span>
<span style="color: #6a9955;"># - Type: SWR = Sequential Write Required</span>

<span style="color: #6a9955;"># Zone management commands</span>
$ nvme zns zone-mgmt-send /dev/nvme0n1 -s 0x0 -a 1   <span style="color: #6a9955;"># Open zone at SLBA 0</span>
$ nvme zns zone-mgmt-send /dev/nvme0n1 -s 0x0 -a 2   <span style="color: #6a9955;"># Close zone</span>
$ nvme zns zone-mgmt-send /dev/nvme0n1 -s 0x0 -a 4   <span style="color: #6a9955;"># Reset zone (erase)</span>
$ nvme zns zone-mgmt-send /dev/nvme0n1 -s 0x0 -a 5   <span style="color: #6a9955;"># Finish zone (mark FULL)</span></code></pre>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># Python ZNS Checkpoint Manager for GPU Training</span>
<span style="color: #6a9955;"># ============================================</span>

<span style="color: #c586c0;">import</span> subprocess
<span style="color: #c586c0;">import</span> os
<span style="color: #c586c0;">from</span> dataclasses <span style="color: #c586c0;">import</span> dataclass
<span style="color: #c586c0;">from</span> typing <span style="color: #c586c0;">import</span> List, Optional

<span style="color: #569cd6;">@dataclass</span>
<span style="color: #c586c0;">class</span> <span style="color: #4ec9b0;">Zone</span>:
    slba: <span style="color: #4ec9b0;">int</span>        <span style="color: #6a9955;"># Starting LBA</span>
    wp: <span style="color: #4ec9b0;">int</span>          <span style="color: #6a9955;"># Write pointer</span>
    capacity: <span style="color: #4ec9b0;">int</span>    <span style="color: #6a9955;"># Zone capacity in sectors</span>
    state: <span style="color: #4ec9b0;">str</span>       <span style="color: #6a9955;"># Zone state</span>

<span style="color: #c586c0;">class</span> <span style="color: #4ec9b0;">ZNSCheckpointManager</span>:
    <span style="color: #ce9178;">"""Manage AI checkpoints on ZNS SSDs with zero WAF"""</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__init__</span>(self, device: <span style="color: #4ec9b0;">str</span>, zone_size_mb: <span style="color: #4ec9b0;">int</span> = <span style="color: #b5cea8;">256</span>):
        self.device = device
        self.zone_size = zone_size_mb * <span style="color: #b5cea8;">1024</span> * <span style="color: #b5cea8;">1024</span>  <span style="color: #6a9955;"># bytes</span>
        self.sector_size = <span style="color: #b5cea8;">512</span>
        self.current_zone_idx = <span style="color: #b5cea8;">0</span>
        self.zones = self._discover_zones()
        
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">_discover_zones</span>(self) -> List[Zone]:
        <span style="color: #ce9178;">"""Parse nvme zns report-zones output"""</span>
        result = subprocess.run(
            [<span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'zns'</span>, <span style="color: #ce9178;">'report-zones'</span>, self.device, <span style="color: #ce9178;">'--output-format=json'</span>],
            capture_output=<span style="color: #569cd6;">True</span>, text=<span style="color: #569cd6;">True</span>
        )
        <span style="color: #c586c0;">import</span> json
        data = json.loads(result.stdout)
        
        zones = []
        <span style="color: #c586c0;">for</span> z <span style="color: #c586c0;">in</span> data.get(<span style="color: #ce9178;">'zones'</span>, []):
            zones.append(Zone(
                slba=z[<span style="color: #ce9178;">'slba'</span>],
                wp=z[<span style="color: #ce9178;">'wp'</span>],
                capacity=z[<span style="color: #ce9178;">'cap'</span>],
                state=z[<span style="color: #ce9178;">'state'</span>]
            ))
        <span style="color: #c586c0;">return</span> zones
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">allocate_zone_for_checkpoint</span>(self, checkpoint_size_bytes: <span style="color: #4ec9b0;">int</span>) -> Optional[<span style="color: #4ec9b0;">int</span>]:
        <span style="color: #ce9178;">"""Find or reset a zone for checkpoint write"""</span>
        
        <span style="color: #6a9955;"># Find EMPTY zone</span>
        <span style="color: #c586c0;">for</span> i, zone <span style="color: #c586c0;">in</span> enumerate(self.zones):
            <span style="color: #c586c0;">if</span> zone.state == <span style="color: #ce9178;">'EMPTY'</span>:
                self._open_zone(zone.slba)
                <span style="color: #c586c0;">return</span> zone.slba
        
        <span style="color: #6a9955;"># No empty zones - find oldest FULL zone and reset</span>
        <span style="color: #c586c0;">for</span> i, zone <span style="color: #c586c0;">in</span> enumerate(self.zones):
            <span style="color: #c586c0;">if</span> zone.state == <span style="color: #ce9178;">'FULL'</span>:
                self._reset_zone(zone.slba)
                self._open_zone(zone.slba)
                <span style="color: #c586c0;">return</span> zone.slba
        
        <span style="color: #c586c0;">return</span> <span style="color: #569cd6;">None</span>  <span style="color: #6a9955;"># No zones available</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">_open_zone</span>(self, slba: <span style="color: #4ec9b0;">int</span>):
        subprocess.run([
            <span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'zns'</span>, <span style="color: #ce9178;">'zone-mgmt-send'</span>, self.device,
            <span style="color: #ce9178;">'-s'</span>, hex(slba), <span style="color: #ce9178;">'-a'</span>, <span style="color: #ce9178;">'1'</span>  <span style="color: #6a9955;"># action=1 is OPEN</span>
        ], check=<span style="color: #569cd6;">True</span>)
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">_reset_zone</span>(self, slba: <span style="color: #4ec9b0;">int</span>):
        subprocess.run([
            <span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'zns'</span>, <span style="color: #ce9178;">'zone-mgmt-send'</span>, self.device,
            <span style="color: #ce9178;">'-s'</span>, hex(slba), <span style="color: #ce9178;">'-a'</span>, <span style="color: #ce9178;">'4'</span>  <span style="color: #6a9955;"># action=4 is RESET</span>
        ], check=<span style="color: #569cd6;">True</span>)
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">write_checkpoint</span>(self, data: <span style="color: #4ec9b0;">bytes</span>, zone_slba: <span style="color: #4ec9b0;">int</span>):
        <span style="color: #ce9178;">"""Write checkpoint data sequentially to zone (via zone-append)"""</span>
        <span style="color: #6a9955;"># In production, use io_uring with zone-append or direct write</span>
        <span style="color: #6a9955;"># This example uses nvme-cli for simplicity</span>
        
        <span style="color: #6a9955;"># Write using zone append (returns actual LBA written)</span>
        <span style="color: #c586c0;">with</span> open(<span style="color: #ce9178;">'/tmp/ckpt_data'</span>, <span style="color: #ce9178;">'wb'</span>) <span style="color: #c586c0;">as</span> f:
            f.write(data)
        
        result = subprocess.run([
            <span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'zns'</span>, <span style="color: #ce9178;">'zone-append'</span>, self.device,
            <span style="color: #ce9178;">'-z'</span>, hex(zone_slba),
            <span style="color: #ce9178;">'-d'</span>, <span style="color: #ce9178;">'/tmp/ckpt_data'</span>
        ], capture_output=<span style="color: #569cd6;">True</span>, text=<span style="color: #569cd6;">True</span>)
        
        <span style="color: #c586c0;">return</span> result.returncode == <span style="color: #b5cea8;">0</span>

<span style="color: #6a9955;"># Usage for AI checkpoint</span>
zns_mgr = ZNSCheckpointManager(<span style="color: #ce9178;">'/dev/nvme0n1'</span>)

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">save_model_checkpoint_zns</span>(model, epoch):
    <span style="color: #c586c0;">import</span> torch
    <span style="color: #c586c0;">import</span> io
    
    <span style="color: #6a9955;"># Serialize model to bytes</span>
    buffer = io.BytesIO()
    torch.save(model.state_dict(), buffer)
    ckpt_data = buffer.getvalue()
    
    <span style="color: #6a9955;"># Allocate zone and write</span>
    zone_slba = zns_mgr.allocate_zone_for_checkpoint(len(ckpt_data))
    <span style="color: #c586c0;">if</span> zone_slba <span style="color: #c586c0;">is not</span> <span style="color: #569cd6;">None</span>:
        zns_mgr.write_checkpoint(ckpt_data, zone_slba)
        print(f<span style="color: #ce9178;">"Checkpoint saved to zone 0x{zone_slba:x}, WAF~1.0 (near-optimal!)"</span>)</code></pre>

            <h3 class="subsection-title">FDP (Flexible Data Placement) Working Examples</h3>
            
            <p>FDP allows applications to hint the SSD about data lifetime, enabling optimal placement and reducing WAF without the complexity of ZNS zone management:</p>
            
            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># FDP SSD Discovery and Configuration</span>
<span style="color: #6a9955;"># ============================================</span>

<span style="color: #6a9955;"># Check if SSD supports FDP</span>
$ nvme id-ctrl /dev/nvme0 | grep -i fdp
fdps     : 1      <span style="color: #6a9955;"># FDP Supported</span>

<span style="color: #6a9955;"># Get FDP configuration</span>
$ nvme fdp configs /dev/nvme0n1
FDP Attributes: 0
Number of Reclaim Groups: 16
Number of Reclaim Unit Handles: 128
Reclaim Unit Nominal Size: 256 MiB
Est. Reclaim Unit Time Limit: 180 s

<span style="color: #6a9955;"># View FDP statistics</span>
$ nvme fdp stats /dev/nvme0n1
Host Bytes Written:        1234567890
Media Bytes Written:       1357913579    <span style="color: #6a9955;"># With FDP: closer to host bytes!</span>
FDP Write Efficiency:      91%           <span style="color: #6a9955;"># Without FDP this would be ~40%</span>

<span style="color: #6a9955;"># List Reclaim Unit Handles</span>
$ nvme fdp usage /dev/nvme0n1
RUHID  0: Written 128 MiB, Utilization 50%
RUHID  1: Written 256 MiB, Utilization 100%  <span style="color: #6a9955;"># Checkpoint data</span>
RUHID  2: Written 64 MiB, Utilization 25%    <span style="color: #6a9955;"># Model weights</span>
RUHID  3: Empty</code></pre>

            <pre><code><span style="color: #6a9955;"># ============================================</span>
<span style="color: #6a9955;"># FDP-Aware Checkpoint Writer (using io_uring)</span>
<span style:="color: #6a9955;"># ============================================</span>

<span style="color: #6a9955;"># FDP works via I/O hints (Directive Streams) in NVMe commands</span>
<span style="color: #6a9955;"># The Placement Handle (RUHID) is passed with each write</span>

<span style="color: #569cd6;">#include</span> &lt;liburing.h&gt;
<span style="color: #569cd6;">#include</span> &lt;linux/nvme_ioctl.h&gt;

<span style="color: #6a9955;">// FDP placement identifiers for different data types</span>
<span style="color: #569cd6;">#define</span> FDP_RUHID_CHECKPOINTS    1   <span style="color: #6a9955;">// Short-lived, replaced frequently</span>
<span style="color: #569cd6;">#define</span> FDP_RUHID_MODEL_WEIGHTS  2   <span style="color: #6a9955;">// Medium-lived</span>
<span style="color: #569cd6;">#define</span> FDP_RUHID_DATASETS       3   <span style="color: #6a9955;">// Long-lived, read-mostly</span>
<span style="color: #569cd6;">#define</span> FDP_RUHID_LOGS           4   <span style="color: #6a9955;">// Append-only</span>

<span style="color: #c586c0;">struct</span> fdp_io_context {
    <span style="color: #c586c0;">struct</span> io_uring ring;
    <span style="color: #4ec9b0;">int</span> fd;
    <span style="color: #4ec9b0;">uint16_t</span> ruhid;  <span style="color: #6a9955;">// Reclaim Unit Handle ID</span>
};

<span style="color: #6a9955;">// Initialize FDP-aware I/O context</span>
<span style="color: #4ec9b0;">int</span> fdp_init(<span style="color: #c586c0;">struct</span> fdp_io_context *ctx, <span style="color: #4ec9b0;">const char</span> *device, <span style="color: #4ec9b0;">uint16_t</span> ruhid) {
    ctx->fd = open(device, O_RDWR | O_DIRECT);
    ctx->ruhid = ruhid;
    
    <span style="color: #6a9955;">// Setup io_uring with fixed files for efficiency</span>
    <span style="color: #c586c0;">struct</span> io_uring_params params = {0};
    params.flags = IORING_SETUP_SQPOLL;  <span style="color: #6a9955;">// Kernel-side polling</span>
    
    <span style="color: #c586c0;">return</span> io_uring_queue_init_params(<span style="color: #b5cea8;">256</span>, &ctx->ring, &params);
}

<span style="color: #6a9955;">// Submit write with FDP placement hint</span>
<span style="color: #4ec9b0;">int</span> fdp_write(<span style="color: #c586c0;">struct</span> fdp_io_context *ctx, <span style="color: #4ec9b0;">void</span> *buf, 
              <span style="color: #4ec9b0;">size_t</span> len, <span style="color: #4ec9b0;">off_t</span> offset) {
    
    <span style="color: #c586c0;">struct</span> io_uring_sqe *sqe = io_uring_get_sqe(&ctx->ring);
    
    <span style="color: #6a9955;">// Prepare write with FDP directive</span>
    io_uring_prep_write(sqe, ctx->fd, buf, len, offset);
    
    <span style="color: #6a9955;">// Set FDP placement hint via write_hint (Linux 6.x+)</span>
    <span style="color: #6a9955;">// The kernel translates this to NVMe Directive Type + Directive Specific</span>
    sqe->write_flags = RWF_WRITE_LIFE_SHORT;  <span style="color: #6a9955;">// For checkpoints</span>
    
    <span style="color: #6a9955;">// For explicit RUHID control (requires nvme passthrough):</span>
    <span style="color: #6a9955;">// sqe->cmd_op = NVME_URING_CMD;</span>
    <span style="color: #6a9955;">// Set Directive Type = 2 (Streams) and DSPEC = ruhid</span>
    
    <span style="color: #c586c0;">return</span> io_uring_submit(&ctx->ring);
}

<span style="color: #6a9955;">// Example: Save checkpoint with FDP hint</span>
<span style="color: #4ec9b0;">void</span> save_checkpoint_fdp(<span style="color: #4ec9b0;">void</span> *model_data, <span style="color: #4ec9b0;">size_t</span> size) {
    <span style="color: #c586c0;">struct</span> fdp_io_context ctx;
    
    <span style="color: #6a9955;">// Use checkpoint-specific RUHID for optimal placement</span>
    fdp_init(&ctx, <span style="color: #ce9178;">"/dev/nvme0n1"</span>, FDP_RUHID_CHECKPOINTS);
    
    <span style="color: #6a9955;">// SSD will place this in reclaim units with other checkpoint data</span>
    <span style="color: #6a9955;">// When checkpoint is replaced, entire RU can be reclaimed efficiently</span>
    fdp_write(&ctx, model_data, size, <span style="color: #b5cea8;">0</span>);
    
    <span style="color: #6a9955;">// Result: WAF approaches 1.0 as checkpoint data isn't mixed</span>
    <span style="color: #6a9955;">// with long-lived data that would cause expensive GC</span>
}</code></pre>

            <div class="info-box">
                <strong>Ã°Å¸Å½Â¯ ZNS vs FDP Decision Matrix:</strong>
                <table class="comparison-table" style="margin-top: 15px;">
                    <thead>
                        <tr>
                            <th>Criteria</th>
                            <th>ZNS</th>
                            <th>FDP</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>WAF Reduction</td>
                            <td class="good">Perfect (1.0)</td>
                            <td class="okay">Excellent (~1.1-1.3)</td>
                        </tr>
                        <tr>
                            <td>Application Changes</td>
                            <td class="warning">Significant (zone mgmt)</td>
                            <td class="good">Minimal (hints only)</td>
                        </tr>
                        <tr>
                            <td>Filesystem Support</td>
                            <td class="warning">btrfs, f2fs only</td>
                            <td class="good">Any (passthrough)</td>
                        </tr>
                        <tr>
                            <td>Existing App Compat</td>
                            <td class="poor">Requires rewrite</td>
                            <td class="good">Graceful degradation</td>
                        </tr>
                        <tr>
                            <td>Best For</td>
                            <td>Greenfield, custom apps</td>
                            <td>Existing apps, containers</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3 class="subsection-title">SSD Endurance Impact</h3>
            
            <pre><code><span style="color: #6a9955;"># Calculate SSD lifespan under AI training workload</span>

<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">estimate_ssd_lifespan</span>(
    ssd_tbw_rating,           <span style="color: #6a9955;"># Rated TBW from spec (e.g., 17,500 for enterprise)</span>
    checkpoint_size_gb,       <span style="color: #6a9955;"># Size per checkpoint (e.g., 100GB)</span>
    checkpoints_per_day,      <span style="color: #6a9955;"># How often (e.g., 24 for hourly)</span>
    waf=<span style="color: #b5cea8;">2.5</span>                   <span style="color: #6a9955;"># Measured WAF</span>
):
    <span style="color: #ce9178;">"""Estimate SSD replacement timeline"""</span>
    
    <span style="color: #6a9955;"># Daily host writes</span>
    daily_writes_tb = (checkpoint_size_gb * checkpoints_per_day) / <span style="color: #b5cea8;">1024</span>
    
    <span style="color: #6a9955;"># Daily NAND writes (what actually wears the SSD)</span>
    daily_nand_writes_tb = daily_writes_tb * waf
    
    <span style="color: #6a9955;"># Years until TBW exhausted</span>
    lifespan_days = ssd_tbw_rating / daily_nand_writes_tb
    lifespan_years = lifespan_days / <span style="color: #b5cea8;">365</span>
    
    print(f<span style="color: #ce9178;">"""
SSD Endurance Analysis:
Ã¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€ÂÃ¢â€Â
Daily host writes:     {daily_writes_tb:.2f} TB
Write amplification:   {waf}Ã—
Daily NAND writes:     {daily_nand_writes_tb:.2f} TB
SSD TBW rating:        {ssd_tbw_rating} TB

Estimated lifespan:    {lifespan_years:.1f} years
Replacement deadline:  {lifespan_years * 0.8:.1f} years (80% threshold)
"""</span>)
    
    <span style="color: #c586c0;">return</span> lifespan_years

<span style="color: #6a9955;"># Example: Enterprise SSD under heavy checkpoint workload</span>
estimate_ssd_lifespan(
    ssd_tbw_rating=<span style="color: #b5cea8;">17500</span>,      <span style="color: #6a9955;"># Intel P5800X 800GB</span>
    checkpoint_size_gb=<span style="color: #b5cea8;">100</span>,
    checkpoints_per_day=<span style="color: #b5cea8;">48</span>,     <span style="color: #6a9955;"># Every 30 min</span>
    waf=<span style="color: #b5cea8;">2.5</span>
)
<span style="color: #6a9955;"># Output: ~2.9 years lifespan at this workload</span></code></pre>

            <div class="info-box warning">
                <strong>Ã¢Å¡Â Ã¯Â¸Â Checkpoint Storm Warning Signs:</strong>
                <ul style="margin-top: 10px; margin-bottom: 0;">
                    <li>P99 write latency spikes 10Ã— during checkpoint windows</li>
                    <li>WAF steadily increasing over weeks (check monthly)</li>
                    <li>Available spare capacity dropping faster than expected</li>
                    <li>Training throughput degrades after 50% SSD capacity used</li>
                </ul>
            </div>
        </section>

        <!-- Section 6: Power & Thermal Management -->
        <section class="section" id="power-management">
            <h2 class="section-title">Ã°Å¸â€Å’ Power & Thermal Management</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â Thermal War Stories:</strong> I've seen million-dollar storage arrays fail because 
                someone blocked an air vent. GPU-storage systems are thermal nightmares â€” 700W GPUs next to 
                25W SSDs that thermal throttle at 70Ã‚Â°C. Here's how to not melt your infrastructure.
            </div>

            <h3 class="subsection-title">NVMe Power States (APST)</h3>
            
            <pre><code><span style="color: #6a9955;"># NVMe Autonomous Power State Transition (APST)</span>

<span style="color: #6a9955;"># List supported power states</span>
nvme id-ctrl /dev/nvme0 | grep -A 20 "ps "
<span style="color: #6a9955;"># ps 0: max_power: 25W, entry_lat: 0Âµs, exit_lat: 0Âµs (operational)</span>
<span style="color: #6a9955;"># ps 1: max_power: 18W, entry_lat: 0Âµs, exit_lat: 0Âµs (operational)</span>
<span style="color: #6a9955;"># ps 2: max_power: 12W, entry_lat: 0Âµs, exit_lat: 0Âµs (operational)</span>
<span style="color: #6a9955;"># ps 3: max_power: 5mW, entry_lat: 1000Âµs, exit_lat: 2000Âµs (non-op)</span>
<span style="color: #6a9955;"># ps 4: max_power: 3mW, entry_lat: 5000Âµs, exit_lat: 10000Âµs (non-op)</span>

<span style="color: #6a9955;"># For GPU workloads: DISABLE low power states</span>
<span style="color: #6a9955;"># The exit latency (10ms for PS4) will kill performance</span>

<span style="color: #6a9955;"># Disable APST entirely</span>
nvme set-feature /dev/nvme0 -f 0x0c -v 0

<span style="color: #6a9955;"># Or set minimum power state to PS2 (operational)</span>
<span style="color: #6a9955;"># This keeps latency acceptable while saving some power</span>
echo 2 > /sys/block/nvme0n1/device/power/pm_qos_latency_tolerance_us

<span style="color: #6a9955;"># Linux kernel APST configuration</span>
<span style="color: #6a9955;"># /etc/modprobe.d/nvme.conf</span>
options nvme default_ps_max_latency_us=0  <span style="color: #6a9955;"># Disable all non-op states</span>

<span style="color: #6a9955;"># For training workloads (bursty I/O):</span>
<span style="color: #6a9955;"># Allow PS2 but not lower (balance power vs latency)</span>
options nvme default_ps_max_latency_us=100

<span style="color: #6a9955;"># Production monitoring for power state issues</span>
<span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">check_nvme_power_state</span>(device):
    <span style="color: #ce9178;">"""Monitor NVMe power state transitions"""</span>
    <span style="color: #c586c0;">import</span> subprocess
    
    result = subprocess.run(
        [<span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'get-feature'</span>, device, <span style="color: #ce9178;">'-f'</span>, <span style="color: #ce9178;">'0x02'</span>, <span style="color: #ce9178;">'-H'</span>],
        capture_output=<span style="color: #569cd6;">True</span>, text=<span style="color: #569cd6;">True</span>
    )
    
    <span style="color: #6a9955;"># Parse current power state</span>
    current_ps = int(result.stdout.split(<span style="color: #ce9178;">'Power State:'</span>)[<span style="color: #b5cea8;">1</span>].split()[<span style="color: #b5cea8;">0</span>])
    
    <span style="color: #c586c0;">if</span> current_ps > <span style="color: #b5cea8;">2</span>:
        print(f<span style="color: #ce9178;">"WARNING: {device} in low-power state PS{current_ps}"</span>)
        print(<span style="color: #ce9178;">"         Next I/O will have high latency!"</span>)
    
    <span style="color: #c586c0;">return</span> current_ps</code></pre>

            <h3 class="subsection-title">Thermal Throttling Detection and Mitigation</h3>
            
            <pre><code><span style="color: #6a9955;"># NVMe thermal monitoring and throttling detection</span>

<span style="color: #6a9955;"># Get thermal thresholds</span>
nvme smart-log /dev/nvme0 | grep -i temp
<span style="color: #6a9955;"># temperature: 45Ã‚Â°C</span>
<span style="color: #6a9955;"># warning_temp_threshold: 70Ã‚Â°C</span>
<span style="color: #6a9955;"># critical_temp_threshold: 80Ã‚Â°C</span>

<span style="color: #6a9955;"># Thermal Management Temperature (TMT) - when throttling kicks in</span>
nvme id-ctrl /dev/nvme0 | grep -i tmt
<span style="color: #6a9955;"># TMT1: 0  (Heavy Throttling Temperature)</span>
<span style="color: #6a9955;"># TMT2: 0  (Light Throttling Temperature)</span>

<span style="color: #6a9955;"># Set thermal throttling thresholds (if configurable)</span>
nvme set-feature /dev/nvme0 -f 0x10 -v 343  <span style="color: #6a9955;"># TMT1 = 70Ã‚Â°C (343K)</span>
nvme set-feature /dev/nvme0 -f 0x10 -v 353  <span style="color: #6a9955;"># TMT2 = 80Ã‚Â°C (353K)</span>

<span style="color: #6a9955;"># Continuous thermal monitoring</span>
<span style="color: #c586c0;">import</span> time
<span style="color: #c586c0;">import</span> subprocess

<span style="color: #c586c0;">class</span> <span style="color: #4ec9b0;">NVMeThermalMonitor</span>:
    <span style="color: #ce9178;">"""Monitor NVMe thermals with throttling detection"""</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">__init__</span>(self, device, warning_temp=<span style="color: #b5cea8;">65</span>, critical_temp=<span style="color: #b5cea8;">75</span>):
        self.device = device
        self.warning_temp = warning_temp
        self.critical_temp = critical_temp
        self.throttle_count = <span style="color: #b5cea8;">0</span>
        self.last_temp = <span style="color: #b5cea8;">0</span>
    
    <span style="color: #c586c0;">def</span> <span style="color: #dcdcaa;">check</span>(self):
        result = subprocess.run(
            [<span style="color: #ce9178;">'nvme'</span>, <span style="color: #ce9178;">'smart-log'</span>, self.device, <span style="color: #ce9178;">'-o'</span>, <span style="color: #ce9178;">'json'</span>],
            capture_output=<span style="color: #569cd6;">True</span>, text=<span style="color: #569cd6;">True</span>
        )
        smart = json.loads(result.stdout)
        
        temp = smart[<span style="color: #ce9178;">'temperature'</span>] - <span style="color: #b5cea8;">273</span>  <span style="color: #6a9955;"># Kelvin to Celsius</span>
        throttle_time = smart.get(<span style="color: #ce9178;">'thm_temp1_trans_count'</span>, <span style="color: #b5cea8;">0</span>)
        
        status = <span style="color: #ce9178;">'OK'</span>
        <span style="color: #c586c0;">if</span> temp >= self.critical_temp:
            status = <span style="color: #ce9178;">'CRITICAL'</span>
        <span style="color: #c586c0;">elif</span> temp >= self.warning_temp:
            status = <span style="color: #ce9178;">'WARNING'</span>
        
        <span style="color: #c586c0;">if</span> throttle_time > self.throttle_count:
            print(f<span style="color: #ce9178;">"ALERT: {self.device} thermal throttling detected!"</span>)
            self.throttle_count = throttle_time
        
        self.last_temp = temp
        <span style="color: #c586c0;">return</span> {<span style="color: #ce9178;">'temp'</span>: temp, <span style="color: #ce9178;">'status'</span>: status, <span style="color: #ce9178;">'throttle_events'</span>: throttle_time}

<span style="color: #6a9955;"># GPU proximity thermal considerations</span>
<span style="color: #ce9178;">"""
Physical Layout Matters:

BAD:
Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â
Ã¢â€â€š  GPU (700W)  Ã¢â€â€š  NVMe  Ã¢â€â€š  Ã¢â€ Â NVMe getting blasted by GPU exhaust
Ã¢â€â€š              Ã¢â€â€š (70Ã‚Â°C) Ã¢â€â€š
Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ

GOOD:
Ã¢â€Å’Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Â
Ã¢â€â€š  NVMe  Ã¢â€â€š              Ã¢â€â€š
Ã¢â€â€š (45Ã‚Â°C) Ã¢â€â€š   GPU (700W) Ã¢â€â€š  Ã¢â€ Â Airflow direction matters
Ã¢â€â€š        Ã¢â€â€š              Ã¢â€â€š
Ã¢â€â€Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€â‚¬Ã¢â€Ëœ

â€¢ SSDs should be UPSTREAM of GPU airflow
â€¢ Minimum 2 slots spacing if possible
â€¢ Use NVMe heatsinks (they actually help)
â€¢ Monitor ambient temp in server room
"""</span></code></pre>

        </section>

        <!-- Section 6.5: Filesystem Configuration for GPU Workloads -->
        <section class="section" id="filesystem-config">
            <h2 class="section-title">ğŸ“ Filesystem Configuration for GPU Workloads</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â Filesystem War Stories:</strong> I've seen 40% performance left on the table because 
                someone used default mkfs options. For GPU workloads with large sequential I/O, stripe alignment 
                and journal configuration make a massive difference. Here's what actually works.
            </div>

            <h3 class="subsection-title">XFS vs ext4: When to Use Each</h3>
            
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Factor</th>
                        <th>XFS</th>
                        <th>ext4</th>
                        <th>Recommendation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Large file performance</td>
                        <td>Excellent (extent-based)</td>
                        <td>Good</td>
                        <td>XFS for checkpoints >1GB</td>
                    </tr>
                    <tr>
                        <td>Parallel I/O</td>
                        <td>Excellent (allocation groups)</td>
                        <td>Moderate</td>
                        <td>XFS for multi-GPU</td>
                    </tr>
                    <tr>
                        <td>Small file overhead</td>
                        <td>Higher metadata cost</td>
                        <td>Lower</td>
                        <td>ext4 for many small files</td>
                    </tr>
                    <tr>
                        <td>Online resize</td>
                        <td>Grow only</td>
                        <td>Grow only</td>
                        <td>Both equivalent</td>
                    </tr>
                    <tr>
                        <td>GDS compatibility</td>
                        <td>Full support</td>
                        <td>Full support</td>
                        <td>Both work</td>
                    </tr>
                    <tr>
                        <td>Repair speed</td>
                        <td>Parallel xfs_repair</td>
                        <td>Single-threaded e2fsck</td>
                        <td>XFS for large volumes</td>
                    </tr>
                </tbody>
            </table>

            <h3 class="subsection-title">XFS mkfs Parameters for GPU Workloads</h3>
            
            <pre><code><span style="color: #6a9955;"># XFS optimal configuration for GPU/AI workloads</span>

<span style="color: #6a9955;"># Single NVMe SSD (e.g., 3.84TB datacenter drive)</span>
mkfs.xfs -f \
    -b size=4096 \                        <span style="color: #6a9955;"># 4K block size (matches NVMe LBA)</span>
    -d agcount=32 \                       <span style="color: #6a9955;"># 32 allocation groups for parallelism</span>
    -l size=1g,lazy-count=1 \             <span style="color: #6a9955;"># 1GB log, lazy superblock counters</span>
    -i maxpct=5 \                         <span style="color: #6a9955;"># Only 5% for inodes (large files)</span>
    /dev/nvme0n1

<span style="color: #6a9955;"># RAID0 array (8x NVMe for checkpoint storage)</span>
<span style="color: #6a9955;"># CRITICAL: stripe unit (su) and stripe width (sw) must match RAID</span>
mkfs.xfs -f \
    -b size=4096 \
    -d su=256k,sw=8 \                     <span style="color: #6a9955;"># 256KB stripe unit Ã— 8 drives = 2MB stripe</span>
    -l su=256k,size=2g,lazy-count=1 \     <span style="color: #6a9955;"># Log also stripe-aligned</span>
    -i maxpct=1 \                         <span style="color: #6a9955;"># 1% inodes for very large files</span>
    /dev/md0

<span style="color: #6a9955;"># Calculation for stripe parameters:</span>
<span style="color: #6a9955;"># su (stripe unit) = chunk size from mdadm --create --chunk=</span>
<span style="color: #6a9955;"># sw (stripe width) = number of data disks (not parity)</span>
<span style="color: #6a9955;"># For RAID0: sw = total disks</span>
<span style="color: #6a9955;"># For RAID5: sw = total disks - 1</span>
<span style="color: #6a9955;"># For RAID6: sw = total disks - 2</span>

<span style="color: #6a9955;"># Large checkpoint files (100GB+) - maximize throughput</span>
mkfs.xfs -f \
    -b size=4096 \
    -d su=1m,sw=8,agcount=64 \            <span style="color: #6a9955;"># 1MB stripe for large sequential I/O</span>
    -l su=1m,size=2g,lazy-count=1 \
    -i maxpct=1 \
    /dev/md0

<span style="color: #6a9955;"># Verify stripe alignment after creation</span>
xfs_info /dev/md0
<span style="color: #6a9955;"># Look for: sunit=X blks, swidth=Y blks</span>
<span style="color: #6a9955;"># sunit should equal su/blocksize = 262144/4096 = 64 blks</span>
<span style="color: #6a9955;"># swidth should equal sw Ã— sunit = 8 Ã— 64 = 512 blks</span></code></pre>

            <h3 class="subsection-title">ext4 mkfs Parameters for GPU Workloads</h3>
            
            <pre><code><span style="color: #6a9955;"># ext4 optimal configuration</span>

<span style="color: #6a9955;"># Single NVMe SSD</span>
mkfs.ext4 -F \
    -b 4096 \                             <span style="color: #6a9955;"># 4K block size</span>
    -E lazy_itable_init=0,lazy_journal_init=0 \  <span style="color: #6a9955;"># Init now, not at runtime</span>
    -E discard \                          <span style="color: #6a9955;"># Enable TRIM during mkfs</span>
    -i 262144 \                           <span style="color: #6a9955;"># One inode per 256KB (large files)</span>
    -J size=1024 \                        <span style="color: #6a9955;"># 1GB journal</span>
    /dev/nvme0n1

<span style="color: #6a9955;"># RAID0 array with stripe alignment</span>
mkfs.ext4 -F \
    -b 4096 \
    -E stride=64,stripe-width=512 \       <span style="color: #6a9955;"># stride=su/bs, stripe-width=strideÃ—disks</span>
    -E lazy_itable_init=0,lazy_journal_init=0 \
    -i 1048576 \                          <span style="color: #6a9955;"># One inode per 1MB</span>
    -J size=2048 \                        <span style="color: #6a9955;"># 2GB journal for large writes</span>
    /dev/md0

<span style="color: #6a9955;"># Calculation for ext4 stride:</span>
<span style="color: #6a9955;"># stride = chunk_size / block_size = 262144 / 4096 = 64</span>
<span style="color: #6a9955;"># stripe-width = stride Ã— number_of_data_disks = 64 Ã— 8 = 512</span></code></pre>

            <h3 class="subsection-title">Mount Options for GPU Workloads</h3>
            
            <pre><code><span style="color: #6a9955;"># XFS mount options for maximum performance</span>
mount -t xfs -o noatime,nodiratime,logbsize=256k,allocsize=1g,inode64 /dev/md0 /mnt/nvme

<span style="color: #6a9955;"># Options explained:</span>
<span style="color: #6a9955;"># noatime      - Don't update access time (saves I/O)</span>
<span style="color: #6a9955;"># nodiratime   - Don't update directory access time</span>
<span style="color: #6a9955;"># logbsize=256k- Log buffer size (match stripe unit)</span>
<span style="color: #6a9955;"># allocsize=1g - Speculative preallocation (HUGE files)</span>
<span style="color: #6a9955;"># inode64      - Allow inodes anywhere (large filesystems)</span>

<span style="color: #6a9955;"># ext4 mount options</span>
mount -t ext4 -o noatime,nodiratime,data=writeback,barrier=0,discard /dev/md0 /mnt/nvme

<span style="color: #6a9955;"># WARNING: data=writeback + barrier=0 = FASTER but UNSAFE</span>
<span style="color: #6a9955;"># Only use if you have UPS and can tolerate data loss on crash</span>
<span style="color: #6a9955;"># For production: data=ordered,barrier=1 (default, safer)</span>

<span style="color: #6a9955;"># /etc/fstab entry for persistent mount</span>
/dev/md0  /mnt/nvme  xfs  defaults,noatime,nodiratime,logbsize=256k,allocsize=1g  0 0</code></pre>

            <h3 class="subsection-title">GDS-Specific Filesystem Requirements</h3>
            
            <pre><code><span style="color: #6a9955;"># GDS requires O_DIRECT support and proper alignment</span>

<span style="color: #6a9955;"># Check if filesystem supports O_DIRECT</span>
python3 -c "
import os
fd = os.open('/mnt/nvme/test', os.O_CREAT | os.O_RDWR | os.O_DIRECT)
print('O_DIRECT supported!')
os.close(fd)
os.remove('/mnt/nvme/test')
"

<span style="color: #6a9955;"># GDS alignment requirements:</span>
<span style="color: #6a9955;"># - Buffer address: 4KB aligned (cuMemAlloc handles this)</span>
<span style="color: #6a9955;"># - File offset: 4KB aligned</span>
<span style="color: #6a9955;"># - Transfer size: 4KB aligned (multiples of 4096)</span>

<span style="color: #6a9955;"># Check filesystem block size matches</span>
stat -f /mnt/nvme | grep "Block size"
<span style="color: #6a9955;"># Should show: Block size: 4096</span>

<span style="color: #6a9955;"># XFS allocation alignment for GDS</span>
xfs_io -c "extsize 4194304" /mnt/nvme  <span style="color: #6a9955;"># 4MB extent hint for large files</span>

<span style="color: #6a9955;"># Verify GDS can use the filesystem</span>
/usr/local/cuda/gds/tools/gdscheck -p
<span style="color: #6a9955;"># Check: "Filesystem [/mnt/nvme] supports GDS: yes"</span></code></pre>

            <div class="info-box warning">
                <strong>Ã¢Å¡Â Ã¯Â¸Â Common Filesystem Mistakes:</strong>
                <ul style="margin-top: 10px; margin-bottom: 0;">
                    <li><strong>Wrong stripe alignment:</strong> 40% performance loss if su/sw don't match RAID</li>
                    <li><strong>Default inode ratio:</strong> Wastes space for large file workloads</li>
                    <li><strong>Small journal:</strong> Causes write stalls during checkpoint storms</li>
                    <li><strong>Missing noatime:</strong> Unnecessary metadata I/O on every read</li>
                    <li><strong>Forgetting discard:</strong> SSD performance degrades over time</li>
                </ul>
            </div>
        </section>

        <!-- Section 7: Cloud Provider Specifics -->
        <section class="section" id="cloud-deployments">
            <h2 class="section-title">Ã¢ËœÂÃ¯Â¸Â Cloud Provider Specifics</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â Cloud Reality:</strong> Cloud providers abstract away the hardware, which means 
                you lose control over NVMe configuration, IOMMU settings, and firmware versions. 
                Here's what you can actually control and how to work around the limitations.
            </div>

            <h3 class="subsection-title">AWS GPU Instances</h3>
            
            <pre><code><span style="color: #6a9955;"># AWS P5/P4d instance storage characteristics</span>

<span style="color: #6a9955;"># P5.48xlarge (H100 Ã— 8)</span>
<span style="color: #6a9955;"># - Instance storage: 8 Ã— 3.84 TB NVMe (30.72 TB total)</span>
<span style="color: #6a9955;"># - Storage bandwidth: ~200 GB/s aggregate</span>
<span style="color: #6a9955;"># - GDS: Supported with NVIDIA driver Ã¢â€°Â¥ 525</span>

<span style="color: #6a9955;"># Optimal AWS P5 storage configuration</span>

<span style="color: #6a9955;"># 1. RAID0 the instance NVMe drives for max bandwidth</span>
sudo mdadm --create /dev/md0 --level=0 --raid-devices=8 \
    /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 \
    /dev/nvme5n1 /dev/nvme6n1 /dev/nvme7n1 /dev/nvme8n1

<span style="color: #6a9955;"># 2. Format with XFS (better for large files than ext4)</span>
sudo mkfs.xfs -f -d su=256k,sw=8 /dev/md0
sudo mount -o noatime,nodiratime,discard /dev/md0 /mnt/nvme

<span style="color: #6a9955;"># 3. Enable GDS</span>
<span style="color: #6a9955;"># AWS AMI with NVIDIA drivers should have GDS ready</span>
sudo modprobe nvidia_fs

<span style="color: #6a9955;"># 4. Verify GDS is working</span>
/usr/local/cuda/gds/tools/gdscheck -p
<span style="color: #6a9955;"># GDS Driver Status: OK</span>
<span style="color: #6a9955;"># Platform compatibility: SUPPORTED</span>

<span style="color: #6a9955;"># AWS-specific limitations:</span>
<span style="color: #6a9955;"># - Cannot change NVMe firmware</span>
<span style="color: #6a9955;"># - Cannot access NVMe-MI</span>
<span style="color: #6a9955;"># - Instance store is EPHEMERAL (lost on stop/terminate)</span>
<span style="color: #6a9955;"># - EBS volumes don't support GDS well (network latency)</span>

<span style="color: #6a9955;"># Best practice: Use instance store for training data, EBS for checkpoints</span>
<span style="color: #6a9955;"># /mnt/nvme  - Training data (fast, ephemeral)</span>
<span style="color: #6a9955;"># /mnt/efs   - Checkpoints (slower, durable)</span></code></pre>

            <h3 class="subsection-title">Azure NDv5 (H100) Configuration</h3>
            
            <pre><code><span style="color: #6a9955;"># Azure ND H100 v5 series</span>

<span style="color: #6a9955;"># Standard_ND96isr_H100_v5 (H100 Ã— 8)</span>
<span style="color: #6a9955;"># - Temp storage: 7.5 TB NVMe</span>
<span style="color: #6a9955;"># - InfiniBand: 400 Gb/s NDR</span>

<span style="color: #6a9955;"># Azure-specific storage setup</span>

<span style="color: #6a9955;"># 1. Find NVMe devices (Azure uses different naming)</span>
lsblk -d | grep nvme

<span style="color: #6a9955;"># 2. Azure Managed Lustre for distributed training</span>
<span style="color: #6a9955;"># Much better than local storage for multi-node</span>
sudo lustre_client_install.sh
sudo mount -t lustre <azure-lustre-ip>@tcp:/lustre /mnt/lustre

<span style="color: #6a9955;"># 3. Azure Blob with BlobFuse2 for checkpoint storage</span>
<span style="color: #6a9955;"># Better durability than local NVMe</span>
blobfuse2 mount /mnt/blob \
    --config-file=/etc/blobfuse2.yaml \
    --disable-writeback-cache=true \
    --file-cache-timeout-in-seconds=0

<span style="color: #6a9955;"># Azure limitations:</span>
<span style="color: #6a9955;"># - Temp storage is local NVMe but capacity varies</span>
<span style="color: #6a9955;"># - No direct GDS support officially documented</span>
<span style="color: #6a9955;"># - Best for IB-connected cluster storage (Lustre)</span></code></pre>

            <h3 class="subsection-title">GCP A3/A2 Configuration</h3>
            
            <pre><code><span style="color: #6a9955;"># GCP A3 (H100 Ã— 8) and A2 (A100 Ã— 16) instances</span>

<span style="color: #6a9955;"># a3-highgpu-8g (H100 Ã— 8)</span>
<span style="color: #6a9955;"># - Local SSD: Up to 6 TB (24 Ã— 375 GB partitions)</span>
<span style="color: #6a9955;"># - Network: 200 Gbps</span>

<span style="color: #6a9955;"># GCP local SSD setup for GPU workloads</span>

<span style="color: #6a9955;"># 1. Create instance with local SSDs</span>
gcloud compute instances create gpu-training \
    --machine-type=a3-highgpu-8g \
    --zone=us-central1-c \
    --local-ssd=interface=NVME \
    --local-ssd=interface=NVME \
    --local-ssd=interface=NVME \
    --local-ssd=interface=NVME \
    --image-family=pytorch-latest-gpu \
    --image-project=deeplearning-platform-release

<span style="color: #6a9955;"># 2. RAID the local SSDs</span>
<span style="color: #6a9955;"># GCP presents them as /dev/nvme0n* </span>
sudo mdadm --create /dev/md0 --level=0 --raid-devices=4 \
    /dev/nvme0n1 /dev/nvme0n2 /dev/nvme0n3 /dev/nvme0n4

<span style="color: #6a9955;"># 3. GCS FUSE for durable storage</span>
gcsfuse --implicit-dirs \
        --file-mode=666 \
        --dir-mode=777 \
        --stat-cache-capacity=1000000 \
        --type-cache-max-size-mb=1024 \
        my-training-bucket /mnt/gcs

<span style="color: #6a9955;"># GCP-specific advantages:</span>
<span style="color: #6a9955;"># - NVIDIA T4/V100/A100/H100 all available</span>
<span style="color: #6a9955;"># - Good GCS integration for checkpoints</span>
<span style="color: #6a9955;"># - Filestore (managed NFS) good for shared data</span>

<span style="color: #6a9955;"># GCP limitations:</span>
<span style="color: #6a9955;"># - Local SSD count/size fixed at instance creation</span>
<span style="color: #6a9955;"># - No GDS official support in documentation</span></code></pre>

            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>AWS P5</th>
                        <th>Azure NDv5</th>
                        <th>GCP A3</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Local NVMe</strong></td>
                        <td class="good">8 Ã— 3.84 TB</td>
                        <td class="medium">~7.5 TB</td>
                        <td class="good">Up to 6 TB</td>
                    </tr>
                    <tr>
                        <td><strong>GDS Support</strong></td>
                        <td class="good">Yes (verified)</td>
                        <td class="medium">Unofficial</td>
                        <td class="medium">Unofficial</td>
                    </tr>
                    <tr>
                        <td><strong>Network Storage</strong></td>
                        <td>EBS, EFS, FSx</td>
                        <td>Managed Lustre, Blob</td>
                        <td>Filestore, GCS</td>
                    </tr>
                    <tr>
                        <td><strong>Best For</strong></td>
                        <td>Large-scale training</td>
                        <td>IB cluster training</td>
                        <td>Flexible workloads</td>
                    </tr>
                </tbody>
            </table>

        </section>

        <!-- Section 8: Next-Gen Hardware Planning -->
        <section class="section" id="next-gen">
            <h2 class="section-title">Ã°Å¸â€Â® Next-Gen Hardware Planning</h2>
            
            <div class="info-box veteran">
                <strong>Ã°Å¸Å½â€“Ã¯Â¸Â Planning Advice:</strong> In 35 years, I've learned that hardware announcements 
                are 50% real and 50% marketing. Blackwell is real, but B200 NVLink configurations won't ship 
                until 2025. Plan for what's available, architect for what's coming.
            </div>

            <h3 class="subsection-title">NVIDIA Blackwell (B100/B200) Storage Implications</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Specification</th>
                        <th>H100 (Current)</th>
                        <th>B100/B200 (Shipping to partners)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>HBM Bandwidth</strong></td>
                        <td>3.35 TB/s</td>
                        <td>8 TB/s</td>
                    </tr>
                    <tr>
                        <td><strong>HBM Capacity</strong></td>
                        <td>80 GB</td>
                        <td>192 GB</td>
                    </tr>
                    <tr>
                        <td><strong>NVLink Bandwidth</strong></td>
                        <td>900 GB/s</td>
                        <td>1.8 TB/s</td>
                    </tr>
                    <tr>
                        <td><strong>Storage Implication</strong></td>
                        <td>10 GB/s adequate</td>
                        <td class="medium">20-50+ GB/s needed (multi-SSD or parallel FS)</td>
                    </tr>
                </tbody>
            </table>

            <div class="info-box warning">
                <strong>Ã¢Å¡Â Ã¯Â¸Â Blackwell Storage Planning:</strong> With Blackwell's 192 GB HBM, model weights fit more comfortably in GPU memory, reducing weight-loading pressure. The main storage bottleneck shifts to checkpoint writes (192 GB per GPU Ã— N GPUs) and dataset prefetch. Plan for multi-SSD RAID or parallel filesystem to achieve 20+ GB/s aggregate throughput per node.
            </div>

            <h3 class="subsection-title">AMD MI325X and MI400 Series</h3>
            
            <pre><code><span style="color: #6a9955;"># AMD MI325X (Available) and MI400 (expected 2026, subject to change) planning</span>

<span style="color: #6a9955;"># MI325X specifications (shipping 2024)</span>
MI325X = {
    <span style="color: #ce9178;">'hbm_capacity'</span>: <span style="color: #ce9178;">'256 GB'</span>,       <span style="color: #6a9955;"># Massive! Reduces storage pressure</span>
    <span style="color: #ce9178;">'hbm_bandwidth'</span>: <span style="color: #ce9178;">'6 TB/s'</span>,
    <span style="color: #ce9178;">'infinity_fabric'</span>: <span style="color: #ce9178;">'896 GB/s'</span>,  <span style="color: #6a9955;"># GPU-GPU interconnect</span>
    <span style="color: #ce9178;">'pcie'</span>: <span style="color: #ce9178;">'Gen5 x16'</span>,            <span style="color: #6a9955;"># 64 GB/s to storage</span>
    <span style="color: #ce9178;">'storage_interface'</span>: <span style="color: #ce9178;">'ROCm + native NVMe'</span>,
}

<span style="color: #6a9955;"># AMD advantage: 256 GB HBM means</span>
<span style="color: #6a9955;"># - Entire 70B model fits in single GPU HBM</span>
<span style="color: #6a9955;"># - Less reliance on NVMe offload</span>
<span style="color: #6a9955;"># - Checkpointing becomes main storage bottleneck</span>

<span style="color: #6a9955;"># MI400 (CDNA 4, expected 2026, subject to change)</span>
<span style="color: #6a9955;"># - Expected 288-384 GB HBM4</span>
<span style="color: #6a9955;"># - CXL 3.0 support likely</span>
<span style="color: #6a9955;"># - UALink interconnect (alternative to NVLink)</span>

<span style="color: #6a9955;"># Storage planning for AMD:</span>
<span style="color: #6a9955;"># 1. ROCm doesn't have GDS equivalent (yet)</span>
<span style="color: #6a9955;"># 2. Focus on large sequential checkpoint writes</span>
<span style="color: #6a9955;"># 3. RAID NVMe arrays still essential</span>
<span style="color: #6a9955;"># 4. Watch for AMD's CXL storage announcements</span></code></pre>

            <h3 class="subsection-title">Storage Technology Roadmap</h3>
            
            <div class="timeline">
                <div class="timeline-item">
                    <span class="timeline-date">2024 - Now</span>
                    <h4 class="timeline-title">PCIe Gen5 NVMe SSDs</h4>
                    <p class="timeline-desc">14+ GB/s sequential, critical for current H100/MI300 deployments. 
                    Samsung PM9A3, Solidigm D7-P5520, Kioxia CM7.</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-date">2025</span>
                    <h4 class="timeline-title">CXL 2.0 Memory Expanders</h4>
                    <p class="timeline-desc">DRAM-backed CXL devices shipping. ~200ns latency. 
                    Samsung CMM-D, Micron CZ120. Good for GPU memory expansion, not storage.</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-date">2025-2026</span>
                    <h4 class="timeline-title">PCIe Gen6 NVMe</h4>
                    <p class="timeline-desc">~28 GB/s sequential. Will help close the GPU-storage gap. 
                    Watch for Samsung, Kioxia announcements.</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-date">2026+</span>
                    <h4 class="timeline-title">CXL 3.0 Storage</h4>
                    <p class="timeline-desc">True storage-class CXL with memory semantics. 
                    Could enable sub-microsecond storage access for GPUs.</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-date">2027+</span>
                    <h4 class="timeline-title">UALink Standard</h4>
                    <p class="timeline-desc">Industry alternative to NVLink. Storage integration unclear 
                    but will likely enable direct GPU-storage protocols.</p>
                </div>
            </div>

            <div class="info-box success">
                <strong>Ã¢Å“â€¦ Planning Recommendations:</strong>
                <ul style="margin-top: 10px; padding-left: 20px;">
                    <li>2024: Deploy Gen5 NVMe RAID arrays (8+ SSDs per GPU node)</li>
                    <li>2025: Evaluate CXL memory for training memory expansion</li>
                    <li>2026: Plan Gen6 NVMe refresh, consider CXL-attached storage</li>
                    <li>Design infrastructure with 4Ã— current bandwidth headroom</li>
                </ul>
            </div>

        </section>

        <nav class="nav-header" style="margin-top: 50px;">
            <a href="09_production_reality.html">Ã¢â€ Â Production Reality</a>
            <span style="color: var(--text-secondary);">Expert Storage Guide</span>
            <a href="11_ml_frameworks.html">ML Frameworks Ã¢â€ â€™</a>
        </nav>

    </div>
</body>
</html>
